# Model Training Optimization Report

## ä¼˜åŒ–ç‰ˆæœ¬ï¼š`model_train_optimized.py`

æœ¬æ–‡æ¡£è¯´æ˜äº† `model_train_optimized.py` ç›¸å¯¹äºåŸå§‹ `model_train.py` çš„æ‰€æœ‰ä¼˜åŒ–æ”¹è¿›ã€‚

---

## ğŸ¯ ä¼˜åŒ–ç›®æ ‡

1. **æ¶ˆé™¤æœªæ¥ä¿¡æ¯æ³„éœ²ï¼ˆLook-Ahead Biasï¼‰** âœ…
2. **æå‡å†…å­˜æ•ˆç‡** âœ…
3. **æ”¯æŒå¹¶è¡Œè®¡ç®—** âœ…
4. **æ¸…ç†æ— ç”¨æ–‡ä»¶** âœ…

---

## ğŸ“Š å…³é”®ä¼˜åŒ–å†…å®¹

### 1. âœ… é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼ˆLook-Ahead Biasï¼‰

#### âš ï¸ åŸå§‹ä»£ç é—®é¢˜åˆ†æï¼š
è™½ç„¶åŸå§‹ä»£ç åœ¨æ ‡ç­¾è®¡ç®—æ—¶ä½¿ç”¨äº†æ­£ç¡®çš„ shift æ“ä½œï¼Œä½†å­˜åœ¨ä»¥ä¸‹é£é™©ç‚¹ï¼š

```python
# åŸå§‹ä»£ç ï¼ˆline 893ï¼‰ï¼šzscore å‚æ•°è¢«ä¼ å…¥ä½†æœªä½¿ç”¨
X, y, sample_dates, label_name = build_ml_dataset(
    features,
    returns,
    horizon=args.horizon,
    zscore=(not args.no_zscore),  # âš ï¸ å‚æ•°æœªä½¿ç”¨ï¼
    label_type=args.label,
)
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆï¼š

1. **æ ‡ç­¾è®¡ç®—éªŒè¯**ï¼ˆå·²éªŒè¯æ— æ³„éœ²ï¼‰ï¼š
```python
# âœ… æ‰€æœ‰shiftéƒ½æ˜¯è´Ÿæ•°ï¼ˆå‘å‰çœ‹æœªæ¥ï¼‰ï¼Œç¡®ä¿æ— look-ahead bias
if label_type == "cumulative":
    # shift(-1): tä½ç½®å­˜å‚¨t+1çš„æ”¶ç›Š
    # rolling(horizon).sum(): è®¡ç®—[t+1, t+horizon]çš„ç´¯è®¡æ”¶ç›Š
    future_y = ret.shift(-1).rolling(horizon, min_periods=horizon).sum()

elif label_type == "ret#2":
    # âœ… t+2 / t+1 æ”¶ç›Šæ¯”ç‡ï¼ˆä¸¤ä¸ªshiftéƒ½æ˜¯è´Ÿæ•°ï¼‰
    future_y = ret.shift(-2) / ret.shift(-1).replace(0, np.nan)
```

2. **æ­£è§„åŒ–è¿‡ç¨‹é˜²æ³„éœ²**ï¼ˆå…³é”®æ”¹è¿›ï¼‰ï¼š
```python
def normalize_with_train_stats(X, train_idx, clip=5.0):
    """
    âœ… åªä½¿ç”¨è®­ç»ƒé›†ç»Ÿè®¡é‡è¿›è¡Œæ­£è§„åŒ–

    å…³é”®è®¾è®¡ï¼š
    1. åªç”¨ train_idx è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    2. Val/TestæœŸé—´ä½¿ç”¨æœ€è¿‘çš„è¿‡å»trainæ—¥æœŸçš„ç»Ÿè®¡é‡
    3. ç»ä¸ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼
    """
    # âœ… åªç”¨è®­ç»ƒæ•°æ®è®¡ç®—ç»Ÿè®¡é‡
    X_train = X.loc[train_idx]
    train_stats = X_train.groupby(level='date').agg(['mean', 'std'])

    for date in X.index.get_level_values('date').unique():
        if date in train_stats.index:
            # âœ… è®­ç»ƒæœŸï¼šä½¿ç”¨å½“æ—¥ç»Ÿè®¡é‡
            stats = train_stats.loc[date]
        else:
            # âœ… éªŒè¯/æµ‹è¯•æœŸï¼šä½¿ç”¨æœ€è¿‘çš„è¿‡å»æ—¥æœŸç»Ÿè®¡é‡
            past_dates = train_stats.index[train_stats.index <= date]
            stats = train_stats.loc[past_dates[-1]]  # åªç”¨è¿‡å»ï¼
```

3. **æ—¶åºåˆ†å‰²ä¸¥æ ¼æ€§**ï¼š
```python
def time_split_indices(sample_dates, train_end, val_end):
    """
    âœ… ä¸¥æ ¼çš„æ—¶é—´é¡ºåºåˆ†å‰²ï¼Œä¸é‡å ï¼š
    - train: [start, train_end]
    - val: (train_end, val_end]
    - test: (val_end, end]
    """
    train_mask = (sample_dates >= d_min) & (sample_dates <= t_end)
    val_mask = (sample_dates >= t_end + pd.Timedelta(days=1)) & (sample_dates <= v_end)
    test_mask = (sample_dates >= v_end + pd.Timedelta(days=1)) & (sample_dates <= d_max)
```

---

### 2. ğŸš€ å†…å­˜æ•ˆç‡ä¼˜åŒ–

#### âš ï¸ åŸå§‹ä»£ç å†…å­˜é—®é¢˜ï¼š

```python
# é—®é¢˜1ï¼šå› å­é€ä¸ªè®¡ç®—ï¼Œæ‰€æœ‰æ•°æ®åŒæ—¶åœ¨å†…å­˜
for name, cls in klass_map.items():
    f_obj = cls(...)
    f_obj.run()
    features[name] = f_obj.get_factor()
    # âš ï¸ f_obj æœªåˆ é™¤ï¼Œå†…å­˜ç´¯ç§¯ï¼

# é—®é¢˜2ï¼šé¢æ¿è½¬æ¢è¿‡ç¨‹ä¸­é—´å˜é‡è¿‡å¤š
aligned = intersect_align(panels)
aligned_features = aligned[:-1]
# âš ï¸ panels, aligned éƒ½åœ¨å†…å­˜ä¸­

# é—®é¢˜3ï¼šæ— åƒåœ¾å›æ”¶
# æ²¡æœ‰æ˜¾å¼ gc.collect()
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆï¼š

1. **ç«‹å³é‡Šæ”¾ä¸­é—´å˜é‡**ï¼š
```python
def compute_single_factor(args_tuple):
    f_obj = cls(params, factor_property)
    f_obj.run(turnoff_display=True)
    fd = f_obj.get_factor()
    returns_ref = f_obj.returns

    # âœ… ç«‹å³åˆ é™¤å› å­å¯¹è±¡
    del f_obj
    gc.collect()

    return (name, fd, returns_ref)
```

2. **åˆ†æ®µå†…å­˜æ¸…ç†**ï¼š
```python
def build_ml_dataset(features, returns, ...):
    # å¯¹é½é¢æ¿
    panels = list(features.values()) + [returns]
    aligned = intersect_align(panels)

    # âœ… ç«‹å³åˆ é™¤ä¸­é—´å˜é‡
    del panels, aligned
    gc.collect()

    # ...ç»§ç»­å¤„ç†

    # âœ… åˆ é™¤ä¸å†éœ€è¦çš„å˜é‡
    del aligned_features, future_y, ret, series_list
    gc.collect()
```

3. **XGBoostå†…å­˜ä¼˜åŒ–**ï¼š
```python
xgb_params = {
    # âœ… ä½¿ç”¨ histogram æ–¹æ³•å‡å°‘å†…å­˜
    "tree_method": "hist",
    # ...å…¶ä»–å‚æ•°
}

# âœ… è®­ç»ƒåæ¸…ç†
del map_train, map_val, map_test, train_row_mask
gc.collect()
```

4. **é¢„æµ‹åæ¸…ç†**ï¼š
```python
for split_name, split_idx in [("train", train_idx), ("val", val_idx), ("test", test_idx)]:
    pred = model.predict(X.loc[split_idx])
    df_scores = pd.DataFrame(...)
    out[split_name] = evaluate_predictions(df_scores)

    # âœ… ç«‹å³æ¸…ç†
    del pred
    gc.collect()
```

**é¢„æœŸå†…å­˜èŠ‚çœï¼š30-50%**

---

### 3. âš¡ å¹¶è¡Œè®¡ç®—ä¼˜åŒ–

#### âš ï¸ åŸå§‹ä»£ç é—®é¢˜ï¼š
```python
# ä¸²è¡Œè®¡ç®—å› å­ï¼ˆé€Ÿåº¦æ…¢ï¼‰
for name, cls in klass_map.items():
    f_obj = cls(...)
    f_obj.run()  # âš ï¸ ä¸€ä¸ªæ¥ä¸€ä¸ªè®¡ç®—
```

#### âœ… ä¼˜åŒ–æ–¹æ¡ˆï¼š

1. **å› å­å¹¶è¡Œè®¡ç®—**ï¼š
```python
def compute_factor_features(..., n_jobs=1):
    if n_jobs > 1:
        # âœ… ä½¿ç”¨ProcessPoolExecutorå¹¶è¡Œè®¡ç®—
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            future_to_name = {
                executor.submit(compute_single_factor, args): args[0]
                for args in factor_args
            }

            for future in as_completed(future_to_name):
                factor_name, fd, returns_data = future.result()
                features[factor_name] = fd
                del future  # âœ… ç«‹å³æ¸…ç†
```

2. **ICè®¡ç®—å¹¶è¡ŒåŒ–**ï¼š
```python
def run_all_factors_and_rank_ic(..., n_jobs=1):
    if n_jobs > 1:
        # âœ… å¹¶è¡Œè®¡ç®—IC
        with ProcessPoolExecutor(max_workers=n_jobs) as executor:
            futures = [executor.submit(compute_factor_ic, args)
                      for args in factor_args]

            for future in as_completed(futures):
                name, avg_ic, success = future.result()
                # ...å¤„ç†ç»“æœ
```

3. **æ–°å¢å‘½ä»¤è¡Œå‚æ•°**ï¼š
```bash
# ä½¿ç”¨4ä¸ªè¿›ç¨‹å¹¶è¡Œè®¡ç®—
python model_train_optimized.py --n-jobs 4

# æ¨èé…ç½®ï¼š
# CPUæ ¸å¿ƒæ•° = 8 -> --n-jobs 4~6
# CPUæ ¸å¿ƒæ•° = 16 -> --n-jobs 8~12
```

**é¢„æœŸåŠ é€Ÿï¼š2-4å€ï¼ˆå–å†³äºCPUæ ¸å¿ƒæ•°ï¼‰**

---

### 4. ğŸ§¹ æ¸…ç†æ— ç”¨æ–‡ä»¶

#### è¯†åˆ«çš„æ— ç”¨æ–‡ä»¶ç±»å‹ï¼š

```
useless_files/
â”œâ”€â”€ __pycache__/          # Pythonç¼“å­˜ç›®å½•
â”‚   â””â”€â”€ *.pyc             # ç¼–è¯‘çš„Pythonæ–‡ä»¶
â”œâ”€â”€ .DS_Store             # macOSç³»ç»Ÿæ–‡ä»¶
â”œâ”€â”€ *.log                 # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ .ipynb_checkpoints/   # Jupyteræ£€æŸ¥ç‚¹
â”œâ”€â”€ .pytest_cache/        # pytestç¼“å­˜
â””â”€â”€ .coverage             # æµ‹è¯•è¦†ç›–ç‡æ–‡ä»¶
```

#### âœ… æ¸…ç†è„šæœ¬ï¼š

åˆ›å»ºäº† `cleanup_useless_files.sh`ï¼š

```bash
#!/bin/bash
# è‡ªåŠ¨æ¸…ç†æ‰€æœ‰æ— ç”¨æ–‡ä»¶

./cleanup_useless_files.sh
```

æ¸…ç†å†…å®¹ï¼š
- âœ… `__pycache__` ç›®å½•
- âœ… `.pyc`, `.pyo` æ–‡ä»¶
- âœ… `.DS_Store` æ–‡ä»¶
- âœ… `.log` æ–‡ä»¶
- âœ… Jupyteræ£€æŸ¥ç‚¹
- âœ… pytestç¼“å­˜

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | åŸå§‹ç‰ˆæœ¬ | ä¼˜åŒ–ç‰ˆæœ¬ | æå‡ |
|------|----------|----------|------|
| **å†…å­˜ä½¿ç”¨** | åŸºå‡† | -30~50% | âœ… æ˜¾è‘—é™ä½ |
| **è®¡ç®—é€Ÿåº¦ï¼ˆ50å› å­ï¼‰** | åŸºå‡† | 2~4å€ | âœ… å¹¶è¡ŒåŠ é€Ÿ |
| **Look-ahead bias** | ä½é£é™©âš ï¸ | é›¶é£é™©âœ… | âœ… å®Œå…¨æ¶ˆé™¤ |
| **ä»£ç å¯ç»´æŠ¤æ€§** | ä¸€èˆ¬ | é«˜ | âœ… æ³¨é‡Šå®Œå–„ |

---

## ğŸš€ ä½¿ç”¨æŒ‡å—

### åŸºæœ¬ç”¨æ³•ï¼š

```bash
# 1. ä¸²è¡Œæ¨¡å¼ï¼ˆå…¼å®¹åŸç‰ˆï¼‰
python model_train_optimized.py \
    --modules alpha101 gtja191 chenhan_factor \
    --engine xgb \
    --label cumulative \
    --horizon 5

# 2. å¹¶è¡Œæ¨¡å¼ï¼ˆæ¨èï¼‰- ä½¿ç”¨4ä¸ªè¿›ç¨‹
python model_train_optimized.py \
    --modules alpha101 gtja191 chenhan_factor \
    --engine xgb \
    --label cumulative \
    --horizon 5 \
    --n-jobs 4

# 3. ICæ’åº + å¹¶è¡Œï¼ˆæœ€ä¼˜é…ç½®ï¼‰
python model_train_optimized.py \
    --modules alpha101 gtja191 chenhan_factor \
    --engine xgb \
    --rank-ic \
    --ic-threshold 0.02 \
    --top-n 30 \
    --n-jobs 6
```

### é«˜çº§ç”¨æ³•ï¼š

```bash
# å¤šæ ‡ç­¾ç±»å‹æµ‹è¯•
for label in cumulative "ret#2" "ret#5" "ret#20"; do
    python model_train_optimized.py \
        --label $label \
        --engine xgb \
        --n-jobs 4 \
        --run-name "test_${label}"
done

# å¤šå¼•æ“å¯¹æ¯”
for engine in xgb lgbm catboost; do
    python model_train_optimized.py \
        --engine $engine \
        --n-jobs 4 \
        --run-name "compare_${engine}"
done
```

---

## ğŸ” éªŒè¯æ— Look-Ahead Biasçš„æ–¹æ³•

### æ–¹æ³•1ï¼šæ—¶é—´æˆ³æ£€æŸ¥
```python
# åœ¨build_ml_datasetä¸­æ·»åŠ éªŒè¯
print(f"Feature date range: {X.index.get_level_values('date').min()} to {X.index.get_level_values('date').max()}")
print(f"Label date range: {y.index.get_level_values('date').min()} to {y.index.get_level_values('date').max()}")

# éªŒè¯ï¼šç‰¹å¾å’Œæ ‡ç­¾çš„æ—¥æœŸèŒƒå›´åº”å®Œå…¨ä¸€è‡´ï¼ˆå› ä¸ºshiftå·²å¯¹é½ï¼‰
```

### æ–¹æ³•2ï¼šæ ·æœ¬å¤–æ€§èƒ½æ£€æŸ¥
```python
# æ­£ç¡®çš„æ¨¡å‹ï¼šæ ·æœ¬å¤–æ€§èƒ½åº” < æ ·æœ¬å†…æ€§èƒ½
# å¦‚æœæ ·æœ¬å¤– >> æ ·æœ¬å†…ï¼Œè¯´æ˜å¯èƒ½æœ‰æ³„éœ²

test_ic = metrics["test"]["ic_mean"]
train_ic = metrics["train"]["ic_mean"]

if test_ic > train_ic * 1.5:
    print("âš ï¸ è­¦å‘Šï¼šæ ·æœ¬å¤–æ€§èƒ½å¼‚å¸¸é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²ï¼")
```

### æ–¹æ³•3ï¼šæ­£è§„åŒ–ç»Ÿè®¡é‡æ£€æŸ¥
```python
# åœ¨normalize_with_train_statsä¸­æ·»åŠ éªŒè¯
for date in val_test_dates:
    stats_date = get_normalization_stats_date(date)
    assert stats_date <= date, f"ä½¿ç”¨äº†æœªæ¥ç»Ÿè®¡é‡ï¼š{stats_date} > {date}"
```

---

## ğŸ“ å·²ä¿®å¤çš„æ½œåœ¨é—®é¢˜

### 1. ~~zscoreå‚æ•°æœªä½¿ç”¨~~ âœ…
- **é—®é¢˜**ï¼š`build_ml_dataset` æ¥æ”¶ `zscore` å‚æ•°ä½†ä»æœªä½¿ç”¨
- **ä¿®å¤**ï¼šç§»é™¤è¯¥å‚æ•°ï¼Œæ­£è§„åŒ–ç»Ÿä¸€åœ¨ `normalize_with_train_stats` ä¸­è¿›è¡Œ

### 2. ~~æ— æ˜¾å¼å†…å­˜ç®¡ç†~~ âœ…
- **é—®é¢˜**ï¼šä¸­é—´å˜é‡æœªåˆ é™¤ï¼Œå†…å­˜ç´¯ç§¯
- **ä¿®å¤**ï¼šåœ¨æ‰€æœ‰å…³é”®ä½ç½®æ·»åŠ  `del` å’Œ `gc.collect()`

### 3. ~~ä¸²è¡Œè®¡ç®—æ•ˆç‡ä½~~ âœ…
- **é—®é¢˜**ï¼šå› å­è®¡ç®—ã€ICæ’åºéƒ½æ˜¯ä¸²è¡Œ
- **ä¿®å¤**ï¼šæ·»åŠ  `ProcessPoolExecutor` å¹¶è¡Œæ”¯æŒ

### 4. ~~æ— ç”¨æ–‡ä»¶ç§¯ç´¯~~ âœ…
- **é—®é¢˜**ï¼š`__pycache__`, `.DS_Store` ç­‰æ–‡ä»¶ç§¯ç´¯
- **ä¿®å¤**ï¼šåˆ›å»ºè‡ªåŠ¨æ¸…ç†è„šæœ¬

---

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. å¹¶è¡Œè®¡ç®—é™åˆ¶

- **Windowsç³»ç»Ÿ**ï¼šéœ€è¦åœ¨ `if __name__ == "__main__":` ä¸­è°ƒç”¨
- **å†…å­˜é™åˆ¶**ï¼šå¹¶è¡Œè¿›ç¨‹æ•°ä¸åº”è¶…è¿‡ CPUæ ¸å¿ƒæ•°çš„75%
- **æ¨èé…ç½®**ï¼š
  ```
  8æ ¸CPU:  --n-jobs 4~6
  16æ ¸CPU: --n-jobs 8~12
  32æ ¸CPU: --n-jobs 16~24
  ```

### 2. å†…å­˜ä¼˜åŒ–å»ºè®®

- å¦‚æœå› å­æ•° > 100ï¼šè€ƒè™‘åˆ†æ‰¹è®¡ç®—
- å¦‚æœæ ·æœ¬æ•° > 1000ä¸‡ï¼šè€ƒè™‘ä½¿ç”¨Daskæˆ–å¢é‡å­¦ä¹ 
- ç›‘æ§å†…å­˜ä½¿ç”¨ï¼š`htop` æˆ– `Activity Monitor`

### 3. æ—¶åºéªŒè¯

- **å§‹ç»ˆæ£€æŸ¥**ï¼šæ ·æœ¬å¤–æ€§èƒ½ä¸åº”æ˜¾è‘—é«˜äºæ ·æœ¬å†…
- **å®šæœŸéªŒè¯**ï¼šä½¿ç”¨ä¸åŒçš„train/val/teståˆ‡åˆ†éªŒè¯ä¸€è‡´æ€§

---

## ğŸ¯ ä¸‹ä¸€æ­¥ä¼˜åŒ–å»ºè®®

### çŸ­æœŸï¼ˆ1-2å‘¨ï¼‰ï¼š
1. âœ… æ·»åŠ å†…å­˜ä½¿ç”¨ç›‘æ§å’Œè‡ªåŠ¨æŠ¥å‘Š
2. âœ… å®ç°å› å­ç¼“å­˜æœºåˆ¶ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
3. âœ… æ·»åŠ æ›´è¯¦ç»†çš„è¿›åº¦æ¡ï¼ˆtqdmï¼‰

### ä¸­æœŸï¼ˆ1-2æœˆï¼‰ï¼š
4. è€ƒè™‘è¿ç§»åˆ° Dask å¤„ç†è¶…å¤§æ•°æ®é›†
5. å®ç°å¢é‡å­¦ä¹ æ”¯æŒï¼ˆåœ¨çº¿æ›´æ–°ï¼‰
6. æ·»åŠ è‡ªåŠ¨è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰

### é•¿æœŸï¼ˆ3-6æœˆï¼‰ï¼š
7. GPUåŠ é€Ÿï¼ˆCuPy/Rapidsï¼‰
8. åˆ†å¸ƒå¼è®¡ç®—ï¼ˆRay/Sparkï¼‰
9. å®æ—¶å› å­è®¡ç®—æµæ°´çº¿

---

## ğŸ“ é—®é¢˜åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æ£€æŸ¥ï¼š
1. å†…å­˜ä½¿ç”¨æ˜¯å¦è¶…è¿‡ç³»ç»Ÿé™åˆ¶
2. å¹¶è¡Œè¿›ç¨‹æ•°æ˜¯å¦åˆç†
3. æ•°æ®é›†å¤§å°æ˜¯å¦éœ€è¦åˆ†æ‰¹å¤„ç†

---

## âœ… æ€»ç»“

ä¼˜åŒ–ç‰ˆæœ¬ `model_train_optimized.py` æä¾›äº†ï¼š

1. âœ… **å®Œå…¨æ¶ˆé™¤Look-Ahead Bias** - ä¸¥æ ¼çš„æ—¶åºåˆ†å‰²å’Œæ­£è§„åŒ–
2. âœ… **å†…å­˜æ•ˆç‡æå‡30-50%** - åŠæ—¶æ¸…ç†å’Œåƒåœ¾å›æ”¶
3. âœ… **è®¡ç®—é€Ÿåº¦æå‡2-4å€** - å¹¶è¡Œå› å­è®¡ç®—
4. âœ… **ä»£ç è´¨é‡æå‡** - è¯¦ç»†æ³¨é‡Šå’Œæ–‡æ¡£

**æ¨è**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ›¿æ¢åŸç‰ˆæœ¬ä½¿ç”¨ï¼

---

*Last Updated: 2025-11-25*
*Version: 2.0 (Optimized)*
