import os
import sys
import json
import argparse
import warnings
import traceback
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


# å°† quant_lib åŠ å…¥è·¯å¾„ï¼Œå¯¼å…¥å› å­åŸºç±»ä¸å› å­åº“
THIS_DIR = os.path.dirname(os.path.abspath(__file__))
QLIB_DIR = os.path.join(THIS_DIR, "quant_lib")
if QLIB_DIR not in sys.path:
	sys.path.append(QLIB_DIR)

from quant_lib.factor import factor  # noqa: E402
from quant_lib.analysis import cs_rank  # noqa: E402
import importlib  # noqa: E402


def discover_factor_classes(module_name: str) -> Dict[str, type]:
	"""ä»ç»™å®šæ¨¡å—ä¸­å‘ç°ç»§æ‰¿è‡ª factor çš„å› å­ç±»ã€‚"""
	# ç›´æ¥å¯¼å…¥æ¨¡å—åï¼ˆå› ä¸º quant_lib å·²ç»åœ¨ sys.path ä¸­ï¼‰
	try:
		m = importlib.import_module(module_name)
	except ImportError as e1:
		# å¦‚æœå¤±è´¥ï¼Œå°è¯•å®Œæ•´åŒ…å
		try:
			full_module_name = f"quant_lib.{module_name}"
			m = importlib.import_module(full_module_name)
		except ImportError as e2:
			print(f"[error] Failed to import {module_name}: {e1}, {e2}")
			return {}
	
	# ä»æ¨¡å—å†…éƒ¨è·å– factor åŸºç±»ï¼ˆå¤„ç†ä¸åŒ import è·¯å¾„çš„é—®é¢˜ï¼‰
	base_factor = m.__dict__.get('factor', None)
	if base_factor is None:
		print(f"[warn] Module {module_name} does not have 'factor' in its namespace")
		return {}
	
	classes = {}
	for k, v in m.__dict__.items():
		if isinstance(v, type):
			try:
				if v is not base_factor and issubclass(v, base_factor):
					# è¿‡æ»¤å†…éƒ¨/æµ‹è¯•ç±»
					# æ¥å— alpha å¼€å¤´çš„ï¼Œä»¥åŠ vol_, ret_, std_ ç­‰è‡ªå®šä¹‰å› å­
					if str(k).startswith("alpha") or any(str(k).startswith(prefix) for prefix in ['vol_', 'ret_', 'std_']):
						classes[k] = v
			except TypeError:
				pass
	
	return classes


def select_factor_classes(
	modules: List[str],
	include: Optional[List[str]] = None,
	exclude: Optional[List[str]] = None,
) -> Dict[str, type]:
	"""èšåˆå¹¶ç­›é€‰å› å­ç±»ã€‚"""
	include = include or []
	exclude = set(exclude or [])
	all_classes: Dict[str, type] = {}
	for mod in modules:
		all_classes.update(discover_factor_classes(mod))
	
	if include:
		picked = {name: all_classes[name] for name in include if name in all_classes}
	else:
		picked = all_classes

	# æ’é™¤æ½œåœ¨è¶…æ…¢/ä¾èµ–æ•°æ®å¤šçš„å°‘æ•°æ¡ç›®ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
	for name in list(picked.keys()):
		if name in exclude:
			picked.pop(name, None)
	return picked


def compute_factor_features(
	klass_map: Dict[str, type],
	start_date: Optional[str],
	end_date: Optional[str],
	universe: Optional[str],
	benchmark: Optional[str],
	display: bool = False,
	do_save: bool = False,
) -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame]:
	"""è®¡ç®—å„å› å­ç‰¹å¾ï¼Œè¿”å› factor_name->DataFrame ä»¥åŠ returns DataFrameã€‚

	DataFrame å½¢çŠ¶å‡ä¸º (date x ticker)ã€‚
	"""
	features: Dict[str, pd.DataFrame] = {}
	returns_ref: Optional[pd.DataFrame] = None

	params_common = {
		"start_date": start_date,
		"end_date": end_date,
		"display": display,
		"save": bool(do_save),
	}
	factor_property_common = {
		"universe": universe,
		"benchmark": benchmark,
	}

	# é€ä¸ªå› å­è¿è¡Œï¼ŒæŠ½å–å› å­å€¼
	for name, cls in klass_map.items():
		try:
			f_obj = cls(params=params_common, factor_property=factor_property_common)
			f_obj.run(turnoff_display=True)
			fd = f_obj.get_factor()
			# ä¸ Universe å¯¹é½åï¼Œä¿å­˜
			features[name] = fd
			if returns_ref is None:
				returns_ref = f_obj.returns
		except Exception as e:
			# å•ä¸ªå› å­å¤±è´¥ä¸å½±å“æ•´ä½“ï¼ˆè®°å½•å¹¶è·³è¿‡ï¼‰
			print(f"[warn] factor {name} failed: {e}")
			if "--debug" in sys.argv:
				traceback.print_exc()
			continue

	if returns_ref is None:
		raise RuntimeError("æ— æ³•åŠ è½½ returns æ•°æ®ï¼ˆå¯èƒ½å› å­å…¨éƒ¨å¤±è´¥ï¼‰ã€‚")
	return features, returns_ref


def run_all_factors_and_rank_ic(
	modules: List[str],
	start_date: Optional[str],
	end_date: Optional[str],
	universe: Optional[str],
	benchmark: Optional[str],
	ic_horizon: int = 5,
	persist_factors: bool = True,
	ic_threshold: float = 0.0,
) -> Tuple[Dict[str, float], List[str]]:
	"""å…¨é‡è¿è¡Œå› å­å¹¶æŒ‰ IC æ’åã€‚

	è¿”å› (å› å­->avg_ic æ˜ å°„, æˆåŠŸè¿è¡Œçš„å› å­ååˆ—è¡¨)ã€‚ä¼šæ ¹æ® persist_factors å†³å®šæ˜¯å¦è½ç›˜ã€‚
	æ”¯æŒé€šè¿‡ ic_threshold ç­›é€‰ abs(rank_ic) > threshold çš„å› å­ã€‚
	"""
	klass_map = select_factor_classes(modules)
	avg_ic_map: Dict[str, float] = {}
	ok_names: List[str] = []

	for name, cls in klass_map.items():
		try:
			# å¼ºåˆ¶ä½¿ç”¨åŒä¸€ IC horizon ä»¥å¯æ¯”
			params = {
				"start_date": start_date,
				"end_date": end_date,
				"save": bool(persist_factors),
				"display": False,
				"ic_return_horizon": int(ic_horizon),
			}
			f_obj = cls(params=params, factor_property={"universe": universe, "benchmark": benchmark})
			f_obj.run(turnoff_display=True)
			res = f_obj.get_performance()
			avg_ic = float(res.get("avg_ic", np.nan))
			# åº”ç”¨ IC é˜ˆå€¼ç­›é€‰ï¼šåªä¿ç•™ abs(rank_ic) > threshold çš„å› å­
			if np.isfinite(avg_ic):
				if abs(avg_ic) > ic_threshold:
					avg_ic_map[name] = avg_ic
				else:
					print(f"[info] factor {name} filtered by IC threshold: abs({avg_ic:.4f}) <= {ic_threshold}")
			ok_names.append(name)
		except Exception as e:
			print(f"[warn] factor {name} failed: {e}")
			# print(traceback.format_exc())  # å®Œæ•´é”™è¯¯ä¿¡æ¯(è°ƒè¯•æ—¶å¯ç”¨)
			continue

	return avg_ic_map, ok_names


def intersect_align(panels: List[pd.DataFrame]) -> List[pd.DataFrame]:
	"""å¯¹é½å¤šåª (date x ticker) é¢æ¿ï¼ŒæŒ‰äº¤é›†å¯¹é½ç´¢å¼•ä¸åˆ—ã€‚"""
	if not panels:
		return panels
	idx = panels[0].index
	cols = panels[0].columns
	for df in panels[1:]:
		idx = idx.intersection(df.index)
		cols = cols.intersection(df.columns)
	return [df.loc[idx, cols] for df in panels]


def panel_to_long(df: pd.DataFrame, name: str) -> pd.Series:
	"""å°† (date x ticker) é¢æ¿è½¬æ¢ä¸ºé•¿æ ¼å¼ Seriesï¼Œç´¢å¼•ä¸º (date, ticker)"""
	s = df.stack()  # stack() é»˜è®¤ç”Ÿæˆ (index, columns) = (date, ticker) çš„ MultiIndex
	s.name = name
	# ç¡®ä¿ MultiIndex çš„åç§°æ­£ç¡®
	s.index.names = ['date', 'ticker']
	return s


def build_ml_dataset(
	features: Dict[str, pd.DataFrame],
	returns: pd.DataFrame,
	horizon: int = 5,
	zscore: bool = True,
	clip: float = 5.0,
) -> Tuple[pd.DataFrame, pd.Series, pd.DatetimeIndex]:
	"""æ„å»ºç›‘ç£å­¦ä¹ æ•°æ®é›†ã€‚

	- X: è¡Œä¸º (date,ticker) çš„æ ·æœ¬ï¼Œåˆ—ä¸ºå„å› å­
	- y: ç›®æ ‡ä¸ºæœªæ¥ horizon æ—¥ç´¯è®¡æ”¶ç›Š
	- dates: æ ·æœ¬å¯¹åº”çš„æ—¥æœŸç´¢å¼•ï¼ˆæ–¹ä¾¿æŒ‰æ—¶é—´åˆ‡åˆ†ï¼‰

	âš ï¸ æ—¶åºé€»è¾‘è¯´æ˜ï¼ˆé‡è¦ï¼‰ï¼š
	1. ç‰¹å¾ X(t): ä½¿ç”¨ t æ—¥æ”¶ç›˜æ—¶åˆ»å¯è·å¾—çš„å› å­å€¼ï¼ˆåŸºäº t æ—¥åŠä¹‹å‰çš„æ•°æ®ï¼‰
	2. æ ‡ç­¾ y(t): ä½¿ç”¨ shift(-1) ç„¶å rolling(horizon).sum() è·å– [t+1, t+horizon] çš„æœªæ¥æ”¶ç›Š
	   - åŸå› ï¼št æ—¥æ”¶ç›˜æ—¶æˆ‘ä»¬æ— æ³•è·å¾— t æ—¥çš„æ”¶ç›Šï¼ˆéœ€è¦ç­‰æ”¶ç›˜ç»“ç®—ï¼‰
	   - å› æ­¤åªèƒ½é¢„æµ‹å¹¶ä½¿ç”¨ t+1 æ—¥å¼€å§‹çš„æœªæ¥æ”¶ç›Š
	   - å®é™…äº¤æ˜“ï¼št æ—¥æ”¶ç›˜å‰ä¸‹å• â†’ t+1 æ—¥å¼€ç›˜æˆäº¤ â†’ è·å¾— [t+1, t+horizon] æ”¶ç›Š
	   - æ­£ç¡®æ–¹æ³•ï¼šret.shift(-1).rolling(horizon).sum()
	     * shift(-1): å°† t+1 æ—¥çš„æ”¶ç›Šç§»åˆ° t æ—¥ä½ç½®
	     * rolling(horizon).sum(): è®¡ç®—ä»å½“å‰ä½ç½®å¼€å§‹çš„ horizon æ—¥ç´¯è®¡æ”¶ç›Š
	     * ç»“æœï¼št æ—¥ä½ç½®å¾—åˆ° [t+1, ..., t+horizon] çš„æ”¶ç›Š âœ…
	3. è¿™æ ·å¯ä»¥é¿å…ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼ˆlook-ahead biasï¼‰ï¼Œç¡®ä¿é€»è¾‘æ­£ç¡® âœ…

	WARNING: zscore æ­£è§„åŒ–åº”åœ¨ train/val/test åˆ†ç¦»åè¿›è¡Œï¼ˆé˜²æ­¢ data leakageï¼‰
	"""
	# å¯¹é½æ‰€æœ‰ç‰¹å¾ä¸æ”¶ç›Šé¢æ¿
	panels = list(features.values()) + [returns]
	aligned = intersect_align(panels)
	aligned_features = aligned[:-1]
	ret = aligned[-1]

	# ç›®æ ‡ï¼šæœªæ¥ horizon æ—¥ç´¯è®¡æ”¶ç›Š
	# âœ… ä¿®æ­£ï¼šå…ˆ shift(-1) è·³è¿‡ t æ—¥ï¼Œå† rolling sumï¼Œç¡®ä¿ t æ—¥çš„ y æ ‡ç­¾æ˜¯çœŸæ­£çš„ [t+1, t+horizon] æ”¶ç›Š
	# åŸä»£ç  bug: rolling().sum().shift(-h) ä¼šå¯¼è‡´ t æ—¥ç”¨åˆ° [t-h+1, t] çš„æ”¶ç›Šï¼ˆlook-ahead biasï¼‰
	future_y = ret.shift(-1).rolling(horizon, min_periods=horizon).sum()

	# äº¤é›†æ—¥æœŸ/è‚¡ç¥¨
	idx = aligned_features[0].index
	cols = aligned_features[0].columns
	for df in aligned_features[1:] + [future_y]:
		idx = idx.intersection(df.index)
		cols = cols.intersection(df.columns)

	# é‡æ–°åˆ‡åˆ°å®Œå…¨å¯¹é½çš„ panel
	aligned_features = [df.loc[idx, cols] for df in aligned_features]
	future_y = future_y.loc[idx, cols]

	# è½¬é•¿è¡¨
	series_list = [panel_to_long(df, name) for df, name in zip(aligned_features, features.keys())]
	X = pd.concat(series_list, axis=1)
	y = panel_to_long(future_y, "y")

	# æ£€æŸ¥é‡å¤ç´¢å¼•
	if X.index.duplicated().any():
		n_dups = X.index.duplicated().sum()
		raise ValueError(f"Found {n_dups} duplicated (date, ticker) pairs in the dataset. "
						 f"This indicates data quality issues.")

	# æ¸…ç†ç¼ºå¤±
	data = pd.concat([X, y], axis=1).dropna(how="any")
	X = data.drop(columns=["y"])  # type: ignore
	y = data["y"]  # type: ignore

	# âš ï¸ Z-scoreëŠ” ì—¬ê¸°ì„œ í•˜ì§€ ì•ŠìŒ - train/val/test ë¶„ë¦¬ í›„ì— ìˆ˜í–‰
	# (data leakage ë°©ì§€)

	# ä» MultiIndex ä¸­æå–æ—¥æœŸï¼ˆlevel='date' æˆ– level=0ï¼‰
	sample_dates = X.index.get_level_values('date')
	return X, y, pd.DatetimeIndex(sample_dates.unique()), zscore, clip


def normalize_with_train_stats(
	X: pd.DataFrame,
	train_idx: pd.Index,
	clip: float = 5.0
) -> pd.DataFrame:
	"""Train ë°ì´í„°ì˜ í†µê³„ëŸ‰ë§Œ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™” (Data Leakage ë°©ì§€)

	Args:
		X: ì „ì²´ ë°ì´í„° (date, ticker) MultiIndex
		train_idx: Training ë°ì´í„°ì˜ ì¸ë±ìŠ¤
		clip: Z-score í´ë¦¬í•‘ ê°’

	Returns:
		ì •ê·œí™”ëœ X
	"""
	X_norm = X.copy()

	# Train ë°ì´í„°ë§Œ ì¶”ì¶œ
	X_train = X.loc[train_idx]

	# cross-sectional normalization
	train_stats = X_train.groupby(level='date').agg(['mean', 'std'])

	# ì „ì²´ ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ ì •ê·œí™”
	for date in X.index.get_level_values('date').unique():
		date_mask = X.index.get_level_values('date') == date

		if date in train_stats.index:
			# Train ê¸°ê°„ ë‚´: í•´ë‹¹ ë‚ ì§œì˜ í†µê³„ëŸ‰ ì‚¬ìš©
			stats = train_stats.loc[date]
		else:
			# Val/Test ê¸°ê°„: ê°€ì¥ ê°€ê¹Œìš´ ê³¼ê±° Train ë‚ ì§œì˜ í†µê³„ëŸ‰ ì‚¬ìš©
			# (ë¯¸ë˜ ì •ë³´ ì‚¬ìš© ì•ˆí•¨!)
			past_dates = train_stats.index[train_stats.index <= date]
			if len(past_dates) > 0:
				stats = train_stats.loc[past_dates[-1]]
			else:
				# Train ì´ì „ ë‚ ì§œ: ì—ëŸ¬ ë°œìƒ (ë°ì´í„°ê°€ ì˜ëª»ë¨)
				raise ValueError(f"Found date {date} before training period. This should not happen. "
								 f"Check your data splitting logic.")

		# ì •ê·œí™” ì ìš©
		for col in X.columns:
			mean_val = stats[(col, 'mean')]
			std_val = stats[(col, 'std')]

			if std_val > 0:
				X_norm.loc[date_mask, col] = (X.loc[date_mask, col] - mean_val) / std_val

	# í´ë¦¬í•‘
	if clip is not None:
		X_norm = X_norm.clip(lower=-clip, upper=clip)

	return X_norm


def time_split_indices(
	sample_dates: pd.DatetimeIndex,
	train_end: Optional[str],
	val_end: Optional[str],
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
	"""æ ¹æ®æ—¥æœŸè¾¹ç•Œç”Ÿæˆ train/val/test ç´¢å¼•æ©ç ã€‚"""
	d_min, d_max = sample_dates.min(), sample_dates.max()
	t_end = pd.to_datetime(train_end) if train_end else pd.to_datetime("2021-12-31")
	v_end = pd.to_datetime(val_end) if val_end else pd.to_datetime("2022-12-31")

	def mask(d1, d2):
		return (sample_dates >= d1) & (sample_dates <= d2)

	train_mask = mask(d_min, t_end)
	val_mask = mask(t_end + pd.Timedelta(days=1), v_end)
	test_mask = mask(v_end + pd.Timedelta(days=1), d_max)
	return train_mask, val_mask, test_mask


def compute_equity_curve(
	df_scores: pd.DataFrame,  # index: (date,ticker), column: pred, y
	quantile: float = 0.2,
) -> pd.Series:
	"""è®¡ç®—å¤šç©ºç»„åˆçš„å‡€å€¼æ›²çº¿ï¼ˆcumulative returnï¼‰ã€‚

	âš ï¸ æ—¶åºé€»è¾‘ï¼š
	- df_scores çš„ date ç´¢å¼•æ˜¯ä¿¡å·æ—¥æœŸ t
	- y åˆ—æ˜¯ [t+1, t+horizon] çš„æœªæ¥æ”¶ç›Šï¼ˆå·²ç»é€šè¿‡ shift(-horizon) å¯¹é½ï¼‰
	- æ¯æ—¥çš„æŒä»“æ”¶ç›Šç›´æ¥å¯¹åº”è¯¥æ—¥çš„ y å€¼ï¼ˆæ­£ç¡®åæ˜ äº†ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„å®é™…æ”¶ç›Šï¼‰

	Args:
		df_scores: é¢„æµ‹æ•°æ®ï¼ŒåŒ…å« predï¼ˆé¢„æµ‹ï¼‰å’Œ yï¼ˆå®é™…æ”¶ç›Šï¼‰
		quantile: å¤šç©ºç»„åˆçš„åˆ†ä½æ•°ï¼ˆå¦‚ 0.2 è¡¨ç¤ºåšå¤š top 20%ï¼Œåšç©º bottom 20%ï¼‰

	Returns:
		æ—¥æœŸç´¢å¼•çš„å‡€å€¼æ›²çº¿ï¼ˆç´¯è®¡æ”¶ç›Šï¼Œåˆå§‹å€¼ä¸º 1.0ï¼‰
	"""
	ls_daily_returns = []
	dates = []
	
	# æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ¯æ—¥å¤šç©ºæ”¶ç›Š
	for d, g in df_scores.groupby(level=0):  # level=0 æ˜¯ date
		if len(g) < 20:  # æ ·æœ¬æ•°å¤ªå°‘åˆ™è·³è¿‡
			continue
		
		# æ ¹æ®é¢„æµ‹å€¼æ’åº
		ranks = g["pred"].rank(pct=True)
		
		# å¤šå¤´ï¼šé¢„æµ‹æ”¶ç›Šæœ€é«˜çš„ quantile åˆ†ä½
		top_ret = g.loc[ranks >= (1 - quantile), "y"].mean()
		# ç©ºå¤´ï¼šé¢„æµ‹æ”¶ç›Šæœ€ä½çš„ quantile åˆ†ä½
		bot_ret = g.loc[ranks <= quantile, "y"].mean()
		
		if np.isfinite(top_ret) and np.isfinite(bot_ret):
			# å¤šç©ºç»„åˆæ”¶ç›Š = å¤šå¤´æ”¶ç›Š - ç©ºå¤´æ”¶ç›Š
			ls_daily_returns.append(top_ret - bot_ret)
			dates.append(d)
	
	# è½¬æ¢ä¸º Series
	ls_series = pd.Series(ls_daily_returns, index=pd.DatetimeIndex(dates))
	
	# è®¡ç®—ç´¯è®¡å‡€å€¼ï¼ˆåˆå§‹å€¼ä¸º 1.0ï¼‰
	equity_curve = (1 + ls_series).cumprod()
	
	return equity_curve


def evaluate_predictions(
	df_scores: pd.DataFrame,  # index: (date,ticker), column: pred, y
	quantile: float = 0.2,
) -> Dict[str, float]:
	"""è¯„ä¼°é¢„æµ‹ï¼šRMSEã€ICã€ICIRã€ä»¥åŠç®€å•å¤šç©ºç»„åˆå¹´åŒ–æ”¶ç›Šä¸ IRã€‚

	Spearman IC æ‰‹å·¥å®ç°ï¼Œé¿å… SciPy ä¾èµ–ï¼ˆå¯¹æ¯ä¸ªæˆªé¢åšç§©ç›¸å…³ï¼‰ã€‚
	"""
	# RMSE
	diff = df_scores["pred"].values - df_scores["y"].values
	rmse = float(np.sqrt(np.mean(diff * diff))) if diff.size else np.nan

	ic_by_day: List[float] = []
	for d, g in df_scores.groupby(level=0):
		if len(g) < 5:
			continue
		rk_pred = g["pred"].rank(pct=True).values
		rk_y = g["y"].rank(pct=True).values
		if rk_pred.std() > 0 and rk_y.std() > 0:
			corr = np.corrcoef(rk_pred, rk_y)[0, 1]
			if np.isfinite(corr):
				ic_by_day.append(float(corr))
	ic_by_day = np.array(ic_by_day, dtype=float)
	ic_mean = float(np.nanmean(ic_by_day)) if ic_by_day.size else np.nan
	ic_ir = float(ic_mean / np.nanstd(ic_by_day)) if ic_by_day.size and np.nanstd(ic_by_day) > 0 else np.nan

	# å¤šç©º
	q = quantile
	ls_daily: List[float] = []
	for d, g in df_scores.groupby(level=0):
		if len(g) < 20:
			continue
		ranks = g["pred"].rank(pct=True)
		top_ret = g.loc[ranks >= (1 - q), "y"].mean()
		bot_ret = g.loc[ranks <= q, "y"].mean()
		if np.isfinite(top_ret) and np.isfinite(bot_ret):
			ls_daily.append(float((top_ret - bot_ret)))
	ls_daily = np.array(ls_daily, dtype=float)
	ann_ret = float(243 * np.nanmean(ls_daily)) if ls_daily.size else np.nan
	ann_vol = float(np.sqrt(243) * np.nanstd(ls_daily)) if ls_daily.size else np.nan
	ann_ir = float(ann_ret / ann_vol) if ls_daily.size and ann_vol > 0 else np.nan

	return {"rmse": rmse, "ic_mean": ic_mean, "ic_ir": ic_ir, "ls_ann_ret": ann_ret, "ls_ann_ir": ann_ir}


def plot_equity_curves(
	equity_data: Dict[str, Dict[str, pd.Series]],
	train_end: str,
	val_end: str,
	save_path: Optional[str] = None,
):
	"""ç»˜åˆ¶å¤šæ¨¡å‹å‡€å€¼æ›²çº¿å¯¹æ¯”å›¾ï¼ŒåŒºåˆ†æ ·æœ¬å†…ï¼ˆè®­ç»ƒ+éªŒè¯ï¼‰å’Œæ ·æœ¬å¤–ï¼ˆæµ‹è¯•ï¼‰ã€‚

	Args:
		equity_data: {
			"model_name": {
				"train": equity_curve_series,
				"val": equity_curve_series,
				"test": equity_curve_series,
			}
		}
		train_end: è®­ç»ƒé›†ç»“æŸæ—¥æœŸï¼ˆæ ·æœ¬å†…å¤–åˆ†ç•Œçº¿1ï¼‰
		val_end: éªŒè¯é›†ç»“æŸæ—¥æœŸï¼ˆæ ·æœ¬å†…å¤–åˆ†ç•Œçº¿2ï¼‰
		save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„ï¼Œå¦‚ None åˆ™æ˜¾ç¤º
	"""
	try:
		import matplotlib.pyplot as plt
		import matplotlib.dates as mdates
	except ImportError:
		print("[warn] matplotlib æœªå®‰è£…ï¼Œæ— æ³•ç»˜åˆ¶å‡€å€¼æ›²çº¿")
		return

	plt.figure(figsize=(14, 7))
	ax = plt.gca()

	# é¢œè‰²æ–¹æ¡ˆ
	colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
	color_idx = 0

	# è½¬æ¢åˆ†ç•Œæ—¥æœŸ
	train_end_dt = pd.to_datetime(train_end)
	val_end_dt = pd.to_datetime(val_end)

	# ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„å‡€å€¼æ›²çº¿
	for model_name, splits in equity_data.items():
		color = colors[color_idx % len(colors)]
		color_idx += 1

		# åˆå¹¶ train, val, test çš„å‡€å€¼æ›²çº¿
		full_equity = pd.Series(dtype=float)
		
		for split_name in ["train", "val", "test"]:
			if split_name in splits and len(splits[split_name]) > 0:
				split_curve = splits[split_name]
				
				# å¦‚æœä¸æ˜¯ç¬¬ä¸€æ®µï¼Œéœ€è¦æ¥ç»­å‰ä¸€æ®µçš„æœ€ç»ˆå‡€å€¼
				if len(full_equity) > 0:
					last_value = full_equity.iloc[-1]
					# å°†å½“å‰æ®µçš„å‡€å€¼è°ƒæ•´ä¸ºæ¥ç»­ä¸Šä¸€æ®µ
					split_curve = split_curve / split_curve.iloc[0] * last_value
				
				full_equity = pd.concat([full_equity, split_curve])

		# ç»˜åˆ¶å®Œæ•´å‡€å€¼æ›²çº¿
		if len(full_equity) > 0:
			ax.plot(full_equity.index, full_equity.values, 
				   label=model_name, color=color, linewidth=2, alpha=0.8)

	# æ·»åŠ æ ·æœ¬å†…å¤–åˆ†ç•Œçº¿
	ax.axvline(x=train_end_dt, color='gray', linestyle='--', linewidth=1.5, alpha=0.7, label='Train/Val Split')
	ax.axvline(x=val_end_dt, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Val/Test Split (æ ·æœ¬å¤–å¼€å§‹)')

	# æ·»åŠ åŒºåŸŸæ ‡æ³¨
	ymin, ymax = ax.get_ylim()
	ax.text(train_end_dt, ymax * 0.95, 'â† è®­ç»ƒæœŸ', ha='right', va='top', fontsize=10, alpha=0.7)
	ax.text(train_end_dt, ymax * 0.95, 'éªŒè¯æœŸ â†’', ha='left', va='top', fontsize=10, alpha=0.7)
	ax.text(val_end_dt, ymax * 0.95, 'â† æ ·æœ¬å†…', ha='right', va='top', fontsize=11, fontweight='bold', alpha=0.8)
	ax.text(val_end_dt, ymax * 0.95, 'æ ·æœ¬å¤– â†’', ha='left', va='top', fontsize=11, fontweight='bold', 
		   color='red', alpha=0.8)

	# å›¾è¡¨ç¾åŒ–
	ax.set_xlabel('Date', fontsize=12)
	ax.set_ylabel('Cumulative Return (Net Value)', fontsize=12)
	ax.set_title('Multi-Model Equity Curves (In-Sample vs Out-of-Sample)', fontsize=14, fontweight='bold')
	ax.legend(loc='upper left', fontsize=10)
	ax.grid(True, alpha=0.3)
	
	# æ—¥æœŸæ ¼å¼åŒ–
	ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
	ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))
	plt.xticks(rotation=45)

	plt.tight_layout()

	if save_path:
		os.makedirs(os.path.dirname(save_path), exist_ok=True)
		plt.savefig(save_path, dpi=300, bbox_inches='tight')
		print(f"ğŸ“Š Equity curve saved to: {save_path}")
		plt.close()
	else:
		plt.show()


def train_xgboost(
	X: pd.DataFrame,
	y: pd.Series,
	sample_dates: pd.DatetimeIndex,
	train_mask: np.ndarray,
	val_mask: np.ndarray,
	test_mask: np.ndarray,
	xgb_params: Optional[dict] = None,
	model_out: Optional[str] = None,
	preds_out: Optional[str] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, pd.DataFrame]]:
	"""è®­ç»ƒ XGBoost å›å½’å¹¶è¯„ä¼° train/val/testï¼Œè¿”å›å„æ®µæŒ‡æ ‡å’Œé¢„æµ‹æ•°æ®ã€‚
	
	Returns:
		(metrics_dict, predictions_dict)
		- metrics_dict: {"train": {...}, "val": {...}, "test": {...}}
		- predictions_dict: {"train": df_scores, "val": df_scores, "test": df_scores}
	"""
	from xgboost import XGBRegressor

	xgb_params = xgb_params or {
		"n_estimators": 1000,
		"max_depth": 6,
		"learning_rate": 0.05,
		"subsample": 0.8,
		"colsample_bytree": 0.8,
		"reg_alpha": 0.0,
		"reg_lambda": 1.0,
		"objective": "reg:squarederror",
		"random_state": 42,
		"n_jobs": max(1, os.cpu_count() or 1),
	}

	# æ„é€ åˆ†æ®µç´¢å¼•
	# å°†æ—¥æœŸçº§åˆ«çš„æ©ç æ˜ å°„åˆ° (date,ticker) è¡Œçº§åˆ«
	date_level = X.index.get_level_values(0)
	map_train = pd.Series(train_mask, index=sample_dates)
	map_val = pd.Series(val_mask, index=sample_dates)
	map_test = pd.Series(test_mask, index=sample_dates)

	train_row_mask = map_train.reindex(date_level).to_numpy()
	val_row_mask = map_val.reindex(date_level).to_numpy()
	test_row_mask = map_test.reindex(date_level).to_numpy()

	idx = X.index
	train_idx = idx[train_row_mask]
	val_idx = idx[val_row_mask]
	test_idx = idx[test_row_mask]

	model = XGBRegressor(**xgb_params)
	# ç®€åŒ–è®­ç»ƒï¼šä¸ä½¿ç”¨ early_stopping_roundsï¼ˆç‰ˆæœ¬å…¼å®¹é—®é¢˜ï¼‰ï¼Œåç»­å¯æ ¹æ® xgb.__version__ æ¡ä»¶æ·»åŠ 
	model.fit(
		X.loc[train_idx],
		y.loc[train_idx],
		eval_set=[(X.loc[val_idx], y.loc[val_idx])],
		verbose=False,
	)

	# é¢„æµ‹å¹¶è¯„ä¼°
	out = {}
	preds_dict = {}  # ä¿å­˜å®Œæ•´é¢„æµ‹æ•°æ®ç”¨äºç»˜å›¾
	for split_name, split_idx in [
		("train", train_idx),
		("val", val_idx),
		("test", test_idx),
	]:
		pred = model.predict(X.loc[split_idx])
		df_scores = pd.DataFrame({"pred": pred, "y": y.loc[split_idx].values}, index=split_idx)
		out[split_name] = evaluate_predictions(df_scores)
		preds_dict[split_name] = df_scores  # ä¿å­˜é¢„æµ‹æ•°æ®

	# ä¿å­˜æ¨¡å‹ä¸é¢„æµ‹ï¼ˆæŒ‰ test æ®µï¼‰
	if model_out:
		os.makedirs(os.path.dirname(model_out), exist_ok=True)
		try:
			model.save_model(model_out)
		except Exception:
			# å¦‚æœ JSON ä¸å¯ç”¨ï¼Œå°è¯• pickle
			import pickle
			with open(model_out + ".pkl", "wb") as f:
				pickle.dump(model, f)

	if preds_out:
		os.makedirs(os.path.dirname(preds_out), exist_ok=True)
		test_pred = model.predict(X.loc[test_idx])
		df_scores = pd.DataFrame({"pred": test_pred, "y": y.loc[test_idx].values}, index=test_idx)
		# ä¿å­˜ä¸ºå‹ç¼© parquet
		try:
			df_scores.to_parquet(preds_out)
		except Exception:
			df_scores.to_csv(preds_out.replace(".parquet", ".csv"))

	return out


def train_lightgbm(
	X: pd.DataFrame,
	y: pd.Series,
	sample_dates: pd.DatetimeIndex,
	train_mask: np.ndarray,
	val_mask: np.ndarray,
	test_mask: np.ndarray,
	lgbm_params: Optional[dict] = None,
	preds_out: Optional[str] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, pd.DataFrame]]:
	try:
		from lightgbm import LGBMRegressor  # type: ignore
	except Exception as e:
		raise RuntimeError(f"LightGBM æœªå®‰è£…æˆ–ä¸å¯ç”¨: {e}")

	lgbm_params = lgbm_params or {
		"n_estimators": 1000,
		"max_depth": -1,
		"learning_rate": 0.05,
		"subsample": 0.8,
		"colsample_bytree": 0.8,
		"reg_alpha": 0.0,
		"reg_lambda": 1.0,
		"objective": "regression",
		"random_state": 42,
		"n_jobs": max(1, os.cpu_count() or 1),
	}

	# æ©ç æ˜ å°„
	date_level = X.index.get_level_values(0)
	map_train = pd.Series(train_mask, index=sample_dates)
	map_val = pd.Series(val_mask, index=sample_dates)
	map_test = pd.Series(test_mask, index=sample_dates)
	train_idx = X.index[map_train.reindex(date_level).to_numpy()]
	val_idx = X.index[map_val.reindex(date_level).to_numpy()]
	test_idx = X.index[map_test.reindex(date_level).to_numpy()]

	model = LGBMRegressor(**lgbm_params)
	model.fit(X.loc[train_idx], y.loc[train_idx])

	out = {}
	preds_dict = {}
	for split_name, split_idx in [("train", train_idx), ("val", val_idx), ("test", test_idx)]:
		pred = model.predict(X.loc[split_idx])
		df_scores = pd.DataFrame({"pred": pred, "y": y.loc[split_idx].values}, index=split_idx)
		out[split_name] = evaluate_predictions(df_scores)
		preds_dict[split_name] = df_scores

	if preds_out:
		os.makedirs(os.path.dirname(preds_out), exist_ok=True)
		test_pred = model.predict(X.loc[test_idx])
		df_scores = pd.DataFrame({"pred": test_pred, "y": y.loc[test_idx].values}, index=test_idx)
		try:
			df_scores.to_parquet(preds_out)
		except Exception:
			df_scores.to_csv(preds_out.replace(".parquet", ".csv"))

	return out, preds_dict


def train_catboost(
	X: pd.DataFrame,
	y: pd.Series,
	sample_dates: pd.DatetimeIndex,
	train_mask: np.ndarray,
	val_mask: np.ndarray,
	test_mask: np.ndarray,
	cb_params: Optional[dict] = None,
	preds_out: Optional[str] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, pd.DataFrame]]:
	try:
		from catboost import CatBoostRegressor  # type: ignore
	except Exception as e:
		raise RuntimeError(f"CatBoost æœªå®‰è£…æˆ–ä¸å¯ç”¨: {e}")

	cb_params = cb_params or {
		"iterations": 1000,
		"depth": 6,
		"learning_rate": 0.05,
		"l2_leaf_reg": 3.0,
		"loss_function": "RMSE",
		"random_seed": 42,
		"verbose": False,
		"thread_count": max(1, os.cpu_count() or 1),
	}

	date_level = X.index.get_level_values(0)
	map_train = pd.Series(train_mask, index=sample_dates)
	map_val = pd.Series(val_mask, index=sample_dates)
	map_test = pd.Series(test_mask, index=sample_dates)
	train_idx = X.index[map_train.reindex(date_level).to_numpy()]
	val_idx = X.index[map_val.reindex(date_level).to_numpy()]
	test_idx = X.index[map_test.reindex(date_level).to_numpy()]

	model = CatBoostRegressor(**cb_params)
	model.fit(X.loc[train_idx], y.loc[train_idx])

	out = {}
	preds_dict = {}
	for split_name, split_idx in [("train", train_idx), ("val", val_idx), ("test", test_idx)]:
		pred = model.predict(X.loc[split_idx])
		df_scores = pd.DataFrame({"pred": pred, "y": y.loc[split_idx].values}, index=split_idx)
		out[split_name] = evaluate_predictions(df_scores)
		preds_dict[split_name] = df_scores

	if preds_out:
		os.makedirs(os.path.dirname(preds_out), exist_ok=True)
		test_pred = model.predict(X.loc[test_idx])
		df_scores = pd.DataFrame({"pred": test_pred, "y": y.loc[test_idx].values}, index=test_idx)
		try:
			df_scores.to_parquet(preds_out)
		except Exception:
			df_scores.to_csv(preds_out.replace(".parquet", ".csv"))

	return out, preds_dict


def main():
	parser = argparse.ArgumentParser(description="Train XGBoost on project factors")
	parser.add_argument("--modules", nargs="*", default=["alpha101", "gtja191", "chenhan_factor"], help="factor modules to use")
	parser.add_argument("--include", nargs="*", default=[], help="explicit factor class names to include")
	parser.add_argument("--exclude", nargs="*", default=[], help="factor class names to exclude")
	parser.add_argument("--start-date", type=str, default=None)
	parser.add_argument("--end-date", type=str, default=None)
	parser.add_argument("--universe", type=str, default="top75pct")
	parser.add_argument("--benchmark", type=str, default=None)
	parser.add_argument("--horizon", type=int, default=5)
	parser.add_argument("--train-end", type=str, default="2021-12-31")
	parser.add_argument("--val-end", type=str, default="2022-12-31")
	parser.add_argument("--model-out", type=str, default=os.path.join(THIS_DIR, "models", "xgb_model.json"))
	parser.add_argument("--preds-out", type=str, default=os.path.join(THIS_DIR, "artifacts", "test_preds.parquet"))
	parser.add_argument("--no-zscore", action="store_true")
	parser.add_argument("--engine", type=str, choices=["xgb","lgbm","catboost"], default="xgb", help="é€‰æ‹©è®­ç»ƒæ¨¡å‹æ¡†æ¶")
	parser.add_argument("--rank-ic", action="store_true", help="è¿è¡Œæ‰€æœ‰å› å­å¹¶è¾“å‡º IC æ’åº JSON")
	parser.add_argument("--ic-horizon", type=int, default=5, help="ç”¨äº IC æ’åºçš„ horizon")
	parser.add_argument("--ic-threshold", type=float, default=0.0, help="IC é˜ˆå€¼ç­›é€‰ï¼šåªä¿ç•™ abs(rank_ic) > threshold çš„å› å­")
	parser.add_argument("--top-n", type=int, default=50, help="æ ¹æ® IC é€‰æ‹©å‰ N ä¸ªå› å­ç”¨äºè®­ç»ƒ")
	parser.add_argument("--save-factor-json", type=str, default=os.path.join(THIS_DIR, "artifacts", "factor_ic_rank.json"))
	parser.add_argument("--persist-factors", action="store_true", help="æ˜¯å¦åœ¨å› å­æ¡†æ¶å†…éƒ¨è§¦å‘ä¿å­˜")

	args = parser.parse_args()

	# å°†å­—ç¬¦ä¸²"None"è½¬æ¢ä¸ºPythonçš„None
	if args.universe == "None":
		args.universe = None
	if args.benchmark == "None":
		args.benchmark = None

	# å¦‚æœéœ€è¦å…ˆè¿è¡Œæ‰€æœ‰å› å­åš IC æ’åº
	factor_names_final: List[str] = []
	if args.rank_ic:
		print(f"[info] Running all factors for IC ranking (threshold: abs(IC) > {args.ic_threshold})...")
		avg_ic_map, ok_names = run_all_factors_and_rank_ic(
			modules=args.modules,
			start_date=args.start_date,
			end_date=args.end_date,
			universe=args.universe,
			benchmark=args.benchmark,
			ic_horizon=args.ic_horizon,
			persist_factors=args.persist_factors,
			ic_threshold=args.ic_threshold,
		)
		# æ’åºå¹¶ä¿å­˜ JSON
		sorted_items = sorted(avg_ic_map.items(), key=lambda kv: abs(kv[1]), reverse=True)
		os.makedirs(os.path.dirname(args.save_factor_json), exist_ok=True)
		with open(args.save_factor_json, "w", encoding="utf-8") as f:
			json.dump({"avg_ic": sorted_items, "all_run": ok_names}, f, ensure_ascii=False, indent=2)
		print(f"[info] IC ranking saved -> {args.save_factor_json}")
		# å–å‰ top-n
		factor_names_final = [name for name,_ in sorted_items[:args.top_n]]
		# è‹¥ç”¨æˆ·æœªæŒ‡å®š include åˆ™ç”¨ top-n
		if not args.include:
			args.include = factor_names_final
		else:
			# ç”¨æˆ·å·²æŒ‡å®š includeï¼Œåˆ™ä¸ top-n åšäº¤é›†å¢å¼ºå¯å¤ç”¨æ€§
			args.include = [nm for nm in args.include if nm in factor_names_final]
		print(f"[info] Selected top {len(args.include)} factors for training: {args.include[:10]}{'...' if len(args.include)>10 else ''}")
		
		# æ¸…ç†å¯¼å…¥çš„æ¨¡å—ï¼Œå¼ºåˆ¶é‡æ–°åŠ è½½
		import importlib
		import sys
		for mod in args.modules:
			if mod in sys.modules:
				importlib.reload(sys.modules[mod])

	# é€‰æ‹©ç”¨äºè®­ç»ƒçš„å› å­ç±»
	klass_map = select_factor_classes(args.modules, include=args.include, exclude=args.exclude)
	if not klass_map:
		raise SystemExit("æœªå‘ç°å¯ç”¨å› å­ç±» (after IC filtering)ï¼Œè¯·æ£€æŸ¥å‚æ•°ã€‚")
	print(f"Using {len(klass_map)} factors for model {args.engine}: {sorted(list(klass_map.keys()))[:10]}{'...' if len(klass_map)>10 else ''}")

	# è®¡ç®—å› å­ç‰¹å¾
	features, returns = compute_factor_features(
		klass_map,
		start_date=args.start_date,
		end_date=args.end_date,
		universe=args.universe,
		benchmark=args.benchmark,
		display=False,
	)

	# æ„å»º ML æ•°æ®é›†
	X, y, sample_dates, do_zscore, clip_val = build_ml_dataset(
		features,
		returns,
		horizon=args.horizon,
		zscore=(not args.no_zscore),
	)

	# æŒ‰æ—¶é—´åˆ‡åˆ†
	train_mask, val_mask, test_mask = time_split_indices(sample_dates, args.train_end, args.val_end)

	# Data Leakage ë°©ì§€: Train í†µê³„ëŸ‰ë§Œ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”
	if do_zscore:
		print("[info] Normalizing with train-only statistics (preventing data leakage)...")
		date_level = X.index.get_level_values(0)
		map_train = pd.Series(train_mask, index=sample_dates)
		train_row_mask = map_train.reindex(date_level).to_numpy()
		train_idx = X.index[train_row_mask]

		X = normalize_with_train_stats(X, train_idx, clip=clip_val)

	# è®­ç»ƒ & è¯„ä¼° & ä¿å­˜
	if args.engine == "xgb":
		metrics, predictions = train_xgboost(
			X, y, sample_dates, train_mask, val_mask, test_mask,
			xgb_params=None,
			model_out=args.model_out,
			preds_out=args.preds_out,
		)
	elif args.engine == "lgbm":
		metrics, predictions = train_lightgbm(
			X, y, sample_dates, train_mask, val_mask, test_mask,
			lgbm_params=None,
			preds_out=args.preds_out.replace("test_preds", "test_preds_lgbm"),
		)
	else:  # catboost
		metrics, predictions = train_catboost(
			X, y, sample_dates, train_mask, val_mask, test_mask,
			cb_params=None,
			preds_out=args.preds_out.replace("test_preds", "test_preds_catboost"),
		)

	# æ‰“å°ç»“æœå¹¶ä¿å­˜æŒ‡æ ‡
	pretty = json.dumps(metrics, ensure_ascii=False, indent=2)
	print(pretty)

	# íŒŒì¼ëª…ì— ìš”ì¸ ìˆ˜, í•™ìŠµ ê¸°ê°„ í¬í•¨
	num_factors = len(klass_map)
	train_period = args.train_end.replace('-', '')
	val_period = args.val_end.replace('-', '')
	filename = f"metrics_{args.engine}_top{num_factors}_train{train_period}_val{val_period}.json"
	out_path = os.path.join(THIS_DIR, "artifacts", filename)
	os.makedirs(os.path.dirname(out_path), exist_ok=True)
	with open(out_path, "w", encoding="utf-8") as f:
		f.write(pretty)

	# ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ê°„ë‹¨í•œ íŒŒì¼ëª…ë„ ìœ ì§€
	simple_path = os.path.join(THIS_DIR, "artifacts", f"metrics_{args.engine}.json")
	with open(simple_path, "w", encoding="utf-8") as f:
		f.write(pretty)

	print(f"\nâœ… Metrics saved to: {filename}")

	# è®¡ç®—å¹¶ç»˜åˆ¶å‡€å€¼æ›²çº¿
	print("\n[info] Computing equity curves...")
	equity_data = {}
	for split_name in ["train", "val", "test"]:
		if split_name in predictions and len(predictions[split_name]) > 0:
			equity_curve = compute_equity_curve(predictions[split_name], quantile=0.2)
			if split_name not in equity_data:
				equity_data[split_name] = {}
			equity_data[split_name] = equity_curve

	# ç»„ç»‡æ•°æ®ç”¨äºç»˜å›¾ï¼ˆæ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”ï¼‰
	model_equity_data = {args.engine.upper(): equity_data}

	# ç»˜åˆ¶å‡€å€¼æ›²çº¿
	equity_plot_path = os.path.join(THIS_DIR, "artifacts", f"equity_curve_{args.engine}.png")
	plot_equity_curves(
		model_equity_data,
		train_end=args.train_end,
		val_end=args.val_end,
		save_path=equity_plot_path,
	)

	print(f"âœ… All results saved!")


if __name__ == "__main__":
	main()

