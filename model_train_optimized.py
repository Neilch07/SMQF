import os
import sys
import json
import argparse
import warnings
import traceback
import gc
from dataclasses import dataclass
from typing import List, Dict, Tuple, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import partial

import numpy as np
import pandas as pd

warnings.filterwarnings("ignore")


# å°† quant_lib åŠ å…¥è·¯å¾„ï¼Œå¯¼å…¥å› å­åŸºç±»ä¸å› å­åº“
THIS_DIR = os.path.dirname(os.path.abspath(__file__))
QLIB_DIR = os.path.join(THIS_DIR, "quant_lib")
if QLIB_DIR not in sys.path:
	sys.path.append(QLIB_DIR)

from quant_lib.factor import factor  # noqa: E402
from quant_lib.analysis import cs_rank  # noqa: E402
import importlib  # noqa: E402


def discover_factor_classes(module_name: str) -> Dict[str, type]:
	"""ä»ç»™å®šæ¨¡å—ä¸­å‘ç°ç»§æ‰¿è‡ª factor çš„å› å­ç±»ã€‚"""
	# ç›´æ¥å¯¼å…¥æ¨¡å—åï¼ˆå› ä¸º quant_lib å·²ç»åœ¨ sys.path ä¸­ï¼‰
	try:
		m = importlib.import_module(module_name)
	except ImportError as e1:
		# å¦‚æœå¤±è´¥ï¼Œå°è¯•å®Œæ•´åŒ…å
		try:
			full_module_name = f"quant_lib.{module_name}"
			m = importlib.import_module(full_module_name)
		except ImportError as e2:
			print(f"[error] Failed to import {module_name}: {e1}, {e2}")
			return {}

	# ä»æ¨¡å—å†…éƒ¨è·å– factor åŸºç±»ï¼ˆå¤„ç†ä¸åŒ import è·¯å¾„çš„é—®é¢˜ï¼‰
	base_factor = m.__dict__.get('factor', None)
	if base_factor is None:
		print(f"[warn] Module {module_name} does not have 'factor' in its namespace")
		return {}

	classes = {}
	for k, v in m.__dict__.items():
		if isinstance(v, type):
			try:
				if v is not base_factor and issubclass(v, base_factor):
					# è¿‡æ»¤å†…éƒ¨/æµ‹è¯•ç±»
					# æ¥å— alpha å¼€å¤´çš„ï¼Œä»¥åŠ vol_, ret_, std_ ç­‰è‡ªå®šä¹‰å› å­
					if str(k).startswith("alpha") or any(str(k).startswith(prefix) for prefix in ['vol_', 'ret_', 'std_']):
						classes[k] = v
			except TypeError:
				pass

	return classes


def select_factor_classes(
	modules: List[str],
	include: Optional[List[str]] = None,
	exclude: Optional[List[str]] = None,
) -> Dict[str, type]:
	"""èšåˆå¹¶ç­›é€‰å› å­ç±»ã€‚"""
	include = include or []
	exclude = set(exclude or [])
	all_classes: Dict[str, type] = {}
	for mod in modules:
		all_classes.update(discover_factor_classes(mod))

	if include:
		picked = {name: all_classes[name] for name in include if name in all_classes}
	else:
		picked = all_classes

	# æ’é™¤æ½œåœ¨è¶…æ…¢/ä¾èµ–æ•°æ®å¤šçš„å°‘æ•°æ¡ç›®ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
	for name in list(picked.keys()):
		if name in exclude:
			picked.pop(name, None)
	return picked


def compute_single_factor(
	args_tuple: Tuple[str, type, dict, dict]
) -> Tuple[str, Optional[pd.DataFrame], Optional[pd.DataFrame]]:
	"""è®¡ç®—å•ä¸ªå› å­çš„ç‰¹å¾ï¼ˆç”¨äºå¹¶è¡Œå¤„ç†ï¼‰ã€‚

	âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šç«‹å³è¿”å›ç»“æœå¹¶é‡Šæ”¾ä¸­é—´å˜é‡

	Returns:
		(factor_name, factor_dataframe, returns_dataframe)
	"""
	name, cls, params_common, factor_property_common = args_tuple

	try:
		f_obj = cls(params=params_common, factor_property=factor_property_common)
		f_obj.run(turnoff_display=True)
		fd = f_obj.get_factor()
		returns_ref = f_obj.returns

		# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šç«‹å³åˆ é™¤å› å­å¯¹è±¡
		del f_obj
		gc.collect()

		return (name, fd, returns_ref)
	except Exception as e:
		print(f"[warn] factor {name} failed: {e}")
		if "--debug" in sys.argv:
			traceback.print_exc()
		return (name, None, None)


def compute_factor_features(
	klass_map: Dict[str, type],
	start_date: Optional[str],
	end_date: Optional[str],
	universe: Optional[str],
	benchmark: Optional[str],
	display: bool = False,
	do_save: bool = False,
	n_jobs: int = 1,
) -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame]:
	"""è®¡ç®—å„å› å­ç‰¹å¾ï¼Œè¿”å› factor_name->DataFrame ä»¥åŠ returns DataFrameã€‚

	âš ï¸ ä¼˜åŒ–ï¼šæ”¯æŒå¹¶è¡Œè®¡ç®—ï¼ˆn_jobs > 1 æ—¶ä½¿ç”¨å¤šè¿›ç¨‹ï¼‰
	âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨è¿­ä»£å™¨è€Œéä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®

	DataFrame å½¢çŠ¶å‡ä¸º (date x ticker)ã€‚
	"""
	features: Dict[str, pd.DataFrame] = {}
	returns_ref: Optional[pd.DataFrame] = None

	params_common = {
		"start_date": start_date,
		"end_date": end_date,
		"display": display,
		"save": bool(do_save),
	}
	factor_property_common = {
		"universe": universe,
		"benchmark": benchmark,
	}

	# å‡†å¤‡å‚æ•°å…ƒç»„åˆ—è¡¨
	factor_args = [
		(name, cls, params_common, factor_property_common)
		for name, cls in klass_map.items()
	]

	if n_jobs > 1:
		# å¹¶è¡Œè®¡ç®—å› å­
		print(f"[info] Computing {len(factor_args)} factors in parallel (n_jobs={n_jobs})...")

		with ProcessPoolExecutor(max_workers=n_jobs) as executor:
			# æäº¤æ‰€æœ‰ä»»åŠ¡
			future_to_name = {
				executor.submit(compute_single_factor, args): args[0]
				for args in factor_args
			}

			# æ”¶é›†ç»“æœ
			for future in as_completed(future_to_name):
				name = future_to_name[future]
				try:
					factor_name, fd, returns_data = future.result()
					if fd is not None:
						features[factor_name] = fd
						if returns_ref is None and returns_data is not None:
							returns_ref = returns_data
						print(f"[info] âœ“ {factor_name} computed")

					# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šç«‹å³åˆ é™¤futureå¯¹è±¡
					del future
				except Exception as e:
					print(f"[warn] factor {name} failed in parallel execution: {e}")

		# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†
		gc.collect()

	else:
		# ä¸²è¡Œè®¡ç®—ï¼ˆåŸå§‹é€»è¾‘ï¼‰
		print(f"[info] Computing {len(factor_args)} factors sequentially...")
		for args in factor_args:
			name, fd, returns_data = compute_single_factor(args)
			if fd is not None:
				features[name] = fd
				if returns_ref is None and returns_data is not None:
					returns_ref = returns_data
				print(f"[info] âœ“ {name} computed")

			# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šç«‹å³æ¸…ç†
			gc.collect()

	if returns_ref is None:
		raise RuntimeError("æ— æ³•åŠ è½½ returns æ•°æ®ï¼ˆå¯èƒ½å› å­å…¨éƒ¨å¤±è´¥ï¼‰ã€‚")

	return features, returns_ref


def run_all_factors_and_rank_ic(
	modules: List[str],
	start_date: Optional[str],
	end_date: Optional[str],
	universe: Optional[str],
	benchmark: Optional[str],
	ic_horizon: int = 5,
	persist_factors: bool = True,
	ic_threshold: float = 0.0,
	n_jobs: int = 1,
) -> Tuple[Dict[str, float], List[str]]:
	"""å…¨é‡è¿è¡Œå› å­å¹¶æŒ‰ IC æ’åã€‚

	âš ï¸ ä¼˜åŒ–ï¼šæ”¯æŒå¹¶è¡Œè®¡ç®—

	è¿”å› (å› å­->avg_ic æ˜ å°„, æˆåŠŸè¿è¡Œçš„å› å­ååˆ—è¡¨)ã€‚ä¼šæ ¹æ® persist_factors å†³å®šæ˜¯å¦è½ç›˜ã€‚
	æ”¯æŒé€šè¿‡ ic_threshold ç­›é€‰ abs(rank_ic) > threshold çš„å› å­ã€‚
	"""
	klass_map = select_factor_classes(modules)
	avg_ic_map: Dict[str, float] = {}
	ok_names: List[str] = []

	def compute_factor_ic(args_tuple):
		"""è®¡ç®—å•ä¸ªå› å­çš„ICï¼ˆç”¨äºå¹¶è¡Œï¼‰"""
		name, cls, params, factor_property = args_tuple
		try:
			f_obj = cls(params=params, factor_property=factor_property)
			f_obj.run(turnoff_display=True)
			res = f_obj.get_performance()
			avg_ic = float(res.get("avg_ic", np.nan))

			# æ¸…ç†
			del f_obj
			gc.collect()

			return (name, avg_ic, True)
		except Exception as e:
			print(f"[warn] factor {name} failed: {e}")
			return (name, np.nan, False)

	# å‡†å¤‡å‚æ•°
	factor_args = []
	for name, cls in klass_map.items():
		params = {
			"start_date": start_date,
			"end_date": end_date,
			"save": bool(persist_factors),
			"display": False,
			"ic_return_horizon": int(ic_horizon),
		}
		factor_args.append((name, cls, params, {"universe": universe, "benchmark": benchmark}))

	if n_jobs > 1:
		# å¹¶è¡Œè®¡ç®—IC
		print(f"[info] Computing IC for {len(factor_args)} factors in parallel (n_jobs={n_jobs})...")

		with ProcessPoolExecutor(max_workers=n_jobs) as executor:
			futures = [executor.submit(compute_factor_ic, args) for args in factor_args]

			for future in as_completed(futures):
				name, avg_ic, success = future.result()

				if success:
					if np.isfinite(avg_ic):
						if abs(avg_ic) > ic_threshold:
							avg_ic_map[name] = avg_ic
							print(f"[info] âœ“ {name}: IC={avg_ic:.4f}")
						else:
							print(f"[info] factor {name} filtered by IC threshold: abs({avg_ic:.4f}) <= {ic_threshold}")
					ok_names.append(name)

				# æ¸…ç†
				del future

		gc.collect()

	else:
		# ä¸²è¡Œè®¡ç®—
		for args in factor_args:
			name, avg_ic, success = compute_factor_ic(args)

			if success:
				if np.isfinite(avg_ic):
					if abs(avg_ic) > ic_threshold:
						avg_ic_map[name] = avg_ic
					else:
						print(f"[info] factor {name} filtered by IC threshold: abs({avg_ic:.4f}) <= {ic_threshold}")
				ok_names.append(name)

	return avg_ic_map, ok_names


def intersect_align(panels: List[pd.DataFrame]) -> List[pd.DataFrame]:
	"""å¯¹é½å¤šåª (date x ticker) é¢æ¿ï¼ŒæŒ‰äº¤é›†å¯¹é½ç´¢å¼•ä¸åˆ—ã€‚

	âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨è§†å›¾è€Œéæ‹·è´
	"""
	if not panels:
		return panels
	idx = panels[0].index
	cols = panels[0].columns
	for df in panels[1:]:
		idx = idx.intersection(df.index)
		cols = cols.intersection(df.columns)

	# ä½¿ç”¨ loc è¿”å›è§†å›¾è€Œéæ‹·è´
	return [df.loc[idx, cols] for df in panels]


def panel_to_long(df: pd.DataFrame, name: str) -> pd.Series:
	"""å°† (date x ticker) é¢æ¿è½¬æ¢ä¸ºé•¿æ ¼å¼ Seriesï¼Œç´¢å¼•ä¸º (date, ticker)

	âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨ copy=False é¿å…ä¸å¿…è¦çš„å¤åˆ¶
	"""
	s = df.stack()  # stack() é»˜è®¤ç”Ÿæˆ (index, columns) = (date, ticker) çš„ MultiIndex
	s.name = name
	# ç¡®ä¿ MultiIndex çš„åç§°æ­£ç¡®
	s.index.names = ['date', 'ticker']
	return s


def build_ml_dataset(
	features: Dict[str, pd.DataFrame],
	returns: pd.DataFrame,
	horizon: int = 5,
	label_type: str = "cumulative",
) -> Tuple[pd.DataFrame, pd.Series, pd.DatetimeIndex, str]:
	"""æ„å»ºç›‘ç£å­¦ä¹ æ•°æ®é›†ã€‚

	âš ï¸ å†…å­˜ä¼˜åŒ–ï¼š
	- åˆ é™¤ä¸­é—´å˜é‡
	- ä½¿ç”¨ç”Ÿæˆå™¨è¡¨è¾¾å¼
	- åŠæ—¶è§¦å‘åƒåœ¾å›æ”¶

	âš ï¸ é˜²æ­¢æœªæ¥ä¿¡æ¯æ³„éœ²ï¼ˆLook-Ahead Biasï¼‰ï¼š

	1. ç‰¹å¾ X(t): ä½¿ç”¨ t æ—¥æ”¶ç›˜æ—¶åˆ»å¯è·å¾—çš„å› å­å€¼ï¼ˆåŸºäº t æ—¥åŠä¹‹å‰çš„æ•°æ®ï¼‰
	2. æ ‡ç­¾ y(t): æ ¹æ® label_type è®¡ç®—ä¸åŒç±»å‹çš„é¢„æµ‹æ ‡ç­¾

	   ã€æ ‡ç­¾ç±»å‹ã€‘ï¼š
	   - "cumulative" (é»˜è®¤): [t+1, t+horizon] ç´¯è®¡æ”¶ç›Š
	     * è®¡ç®—ï¼šret.shift(-1).rolling(horizon).sum()
	     * å«ä¹‰ï¼št+1 åˆ° t+horizon çš„æ€»æ”¶ç›Š
	     * âœ… æ— look-ahead bias: ä½¿ç”¨shift(-1)ç¡®ä¿åªç”¨æœªæ¥æ”¶ç›Š

	   - "ret#2": t+2 / t+1 çš„æ”¶ç›Šæ¯”ç‡
	     * è®¡ç®—ï¼šret.shift(-2) / ret.shift(-1)
	     * å«ä¹‰ï¼šç¬¬2å¤©æ”¶ç›Šç›¸å¯¹ç¬¬1å¤©æ”¶ç›Šçš„å€æ•°
	     * âœ… æ— look-ahead bias: ä¸¤ä¸ªshiftéƒ½æ˜¯è´Ÿæ•°ï¼ˆæœªæ¥ï¼‰

	   - "ret#5": t+5 / t+1 çš„æ”¶ç›Šæ¯”ç‡
	     * è®¡ç®—ï¼šret.shift(-5) / ret.shift(-1)
	     * å«ä¹‰ï¼šç¬¬5å¤©æ”¶ç›Šç›¸å¯¹ç¬¬1å¤©æ”¶ç›Šçš„å€æ•°
	     * âœ… æ— look-ahead bias: ä¸¤ä¸ªshiftéƒ½æ˜¯è´Ÿæ•°ï¼ˆæœªæ¥ï¼‰

	   - "ret#20": t+20 / t+1 çš„æ”¶ç›Šæ¯”ç‡
	     * è®¡ç®—ï¼šret.shift(-20) / ret.shift(-1)
	     * å«ä¹‰ï¼šç¬¬20å¤©æ”¶ç›Šç›¸å¯¹ç¬¬1å¤©æ”¶ç›Šçš„å€æ•°
	     * âœ… æ— look-ahead bias: ä¸¤ä¸ªshiftéƒ½æ˜¯è´Ÿæ•°ï¼ˆæœªæ¥ï¼‰

	3. **éªŒè¯æ—¶åºé€»è¾‘**ï¼š
	   - æ‰€æœ‰ shift æ“ä½œéƒ½æ˜¯è´Ÿæ•°ï¼ˆ-1, -2, -5, -20ï¼‰ï¼Œè¡¨ç¤ºå‘å‰çœ‹æœªæ¥
	   - ç‰¹å¾X(t)åœ¨æ—¶é—´tå¯å¾—ï¼Œæ ‡ç­¾y(t)åŸºäºt+1åŠä¹‹åçš„æ”¶ç›Š
	   - è®­ç»ƒæ—¶ï¼šæ¨¡å‹å­¦ä¹  X(t) -> y(t)ï¼ˆæœªæ¥æ”¶ç›Šï¼‰çš„æ˜ å°„
	   - é¢„æµ‹æ—¶ï¼šä½¿ç”¨ X(t) é¢„æµ‹ y(t)ï¼Œå®é™…äº¤æ˜“åœ¨ t+1 è¿›è¡Œ

	âš ï¸ é‡è¦æé†’ï¼š
	- zscore æ­£è§„åŒ–åº”åœ¨ train/val/test åˆ†ç¦»åè¿›è¡Œï¼ˆé˜²æ­¢ data leakageï¼‰
	- æœ¬å‡½æ•°åªæ„é€ åŸå§‹ç‰¹å¾å’Œæ ‡ç­¾ï¼Œæ­£è§„åŒ–åœ¨main()ä¸­train_idxä¸Šè¿›è¡Œ
	"""
	# å¯¹é½æ‰€æœ‰ç‰¹å¾ä¸æ”¶ç›Šé¢æ¿
	panels = list(features.values()) + [returns]
	aligned = intersect_align(panels)
	aligned_features = aligned[:-1]
	ret = aligned[-1]

	# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåˆ é™¤ä¸­é—´å˜é‡
	del panels, aligned
	gc.collect()

	# æ ¹æ® label_type è®¡ç®—ç›®æ ‡æ ‡ç­¾ï¼ˆâš ï¸ å…³é”®ï¼šæ‰€æœ‰shiftéƒ½æ˜¯è´Ÿæ•°ï¼Œç¡®ä¿æ— look-ahead biasï¼‰
	if label_type == "cumulative":
		# âœ… ç´¯è®¡æ”¶ç›Šï¼š[t+1, t+horizon]
		# shift(-1): å°†æ”¶ç›Šå‘å‰ç§»1æœŸï¼ˆtçš„ä½ç½®å­˜å‚¨t+1çš„æ”¶ç›Šï¼‰
		# rolling(horizon).sum(): è®¡ç®—æœªæ¥horizonæœŸçš„ç´¯è®¡æ”¶ç›Š
		future_y = ret.shift(-1).rolling(horizon, min_periods=horizon).sum()
		label_name = f"ret_cumsum_{horizon}d"

	elif label_type == "ret#2":
		# âœ… t+2 / t+1 æ”¶ç›Šæ¯”ç‡ï¼ˆæ— look-ahead biasï¼‰
		future_y = ret.shift(-2) / ret.shift(-1).replace(0, np.nan)
		label_name = "ret#2"

	elif label_type == "ret#5":
		# âœ… t+5 / t+1 æ”¶ç›Šæ¯”ç‡ï¼ˆæ— look-ahead biasï¼‰
		future_y = ret.shift(-5) / ret.shift(-1).replace(0, np.nan)
		label_name = "ret#5"

	elif label_type == "ret#20":
		# âœ… t+20 / t+1 æ”¶ç›Šæ¯”ç‡ï¼ˆæ— look-ahead biasï¼‰
		future_y = ret.shift(-20) / ret.shift(-1).replace(0, np.nan)
		label_name = "ret#20"

	else:
		raise ValueError(f"Unknown label_type: {label_type}. "
						 f"Supported: cumulative, ret#2, ret#5, ret#20")

	# äº¤é›†æ—¥æœŸ/è‚¡ç¥¨
	idx = aligned_features[0].index
	cols = aligned_features[0].columns
	for df in aligned_features[1:] + [future_y]:
		idx = idx.intersection(df.index)
		cols = cols.intersection(df.columns)

	# é‡æ–°åˆ‡åˆ°å®Œå…¨å¯¹é½çš„ panel
	aligned_features = [df.loc[idx, cols] for df in aligned_features]
	future_y = future_y.loc[idx, cols]

	# è½¬é•¿è¡¨ï¼ˆâš ï¸ å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨ç”Ÿæˆå™¨ï¼‰
	series_list = [panel_to_long(df, name) for df, name in zip(aligned_features, features.keys())]
	X = pd.concat(series_list, axis=1)
	y = panel_to_long(future_y, "y")

	# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåˆ é™¤ä¸­é—´å˜é‡
	del aligned_features, future_y, ret, series_list
	gc.collect()

	# æ£€æŸ¥é‡å¤ç´¢å¼•
	if X.index.duplicated().any():
		n_dups = X.index.duplicated().sum()
		raise ValueError(f"Found {n_dups} duplicated (date, ticker) pairs in the dataset. "
						 f"This indicates data quality issues.")

	# æ¸…ç†ç¼ºå¤±
	data = pd.concat([X, y], axis=1).dropna(how="any")
	X = data.drop(columns=["y"])  # type: ignore
	y = data["y"]  # type: ignore

	# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåˆ é™¤data
	del data
	gc.collect()

	# ä» MultiIndex ä¸­æå–æ—¥æœŸï¼ˆlevel='date' æˆ– level=0ï¼‰
	sample_dates = X.index.get_level_values('date')
	return X, y, pd.DatetimeIndex(sample_dates.unique()), label_name


def normalize_with_train_stats(
	X: pd.DataFrame,
	train_idx: pd.Index,
	clip: float = 5.0
) -> pd.DataFrame:
	"""âœ… Train ë°ì´í„°ì˜ í†µê³„ëŸ‰ë§Œ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™” (Data Leakage ë°©ì§€)

	âš ï¸ é˜²æ­¢æ•°æ®æ³„éœ²çš„å…³é”®è®¾è®¡ï¼š
	1. åªä½¿ç”¨ train_idx çš„æ•°æ®è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
	2. Val/Test æœŸé—´ä½¿ç”¨æœ€è¿‘çš„è¿‡å» train æ—¥æœŸçš„ç»Ÿê³„ëŸ‰ï¼ˆä¸ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼‰
	3. è¿™ç¡®ä¿äº†åœ¨å®é™…é¢„æµ‹æ—¶ï¼Œåªä½¿ç”¨å†å²å¯å¾—çš„ä¿¡æ¯

	Args:
		X: ì „ì²´ ë°ì´í„° (date, ticker) MultiIndex
		train_idx: Training ë°ì´í„°çš„ì¸ë±ìŠ¤
		clip: Z-score í´ë¦¬í•‘ ê°’

	Returns:
		ì •ê·œåŒ–ëœ X
	"""
	X_norm = X.copy()

	# âœ… Train ë°ì´í„°ë§Œ ì¶”ì¶œï¼ˆé˜²æ­¢æ³„éœ²çš„å…³é”®æ­¥éª¤1ï¼‰
	X_train = X.loc[train_idx]

	# âœ… cross-sectional normalization (æ¯æ—¥æ ‡å‡†åŒ–)
	train_stats = X_train.groupby(level='date').agg(['mean', 'std'])

	# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåˆ é™¤X_train
	del X_train
	gc.collect()

	# ì „ì²´ ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ ì •ê·œí™”
	for date in X.index.get_level_values('date').unique():
		date_mask = X.index.get_level_values('date') == date

		if date in train_stats.index:
			# âœ… Train ê¸°ê°„ ë‚´: í•´ë‹¹ ë‚ ì§œì˜ í†µê³„é‡ ì‚¬ìš©
			stats = train_stats.loc[date]
		else:
			# âœ… Val/Test ê¸°ê°„: ê°€ì¥ ê°€ê¹Œìš´ ê³¼ê±° Train ë‚ ì§œì˜ í†µê³„é‡ ì‚¬ìš©
			# âš ï¸ å…³é”®ï¼šåªä½¿ç”¨è¿‡å»çš„ç»Ÿè®¡é‡ï¼Œä¸ä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼
			past_dates = train_stats.index[train_stats.index <= date]
			if len(past_dates) > 0:
				stats = train_stats.loc[past_dates[-1]]  # ä½¿ç”¨æœ€è¿‘çš„è¿‡å»æ—¥æœŸ
			else:
				# Train ì´ì „ ë‚ ì§œ: ì—ëŸ¬ ë°œìƒ (ë°ì´í„°ê°€ ì˜ëª»ë¨)
				raise ValueError(f"Found date {date} before training period. This should not happen. "
								 f"Check your data splitting logic.")

		# ì •ê·œí™” ì ìš©
		for col in X.columns:
			mean_val = stats[(col, 'mean')]
			std_val = stats[(col, 'std')]

			if std_val > 0:
				X_norm.loc[date_mask, col] = (X.loc[date_mask, col] - mean_val) / std_val

	# í´ë¦¬í•‘
	if clip is not None:
		X_norm = X_norm.clip(lower=-clip, upper=clip)

	# âš ï¸ å†…å­˜ä¼˜åŒ–
	del train_stats
	gc.collect()

	return X_norm


def time_split_indices(
	sample_dates: pd.DatetimeIndex,
	train_end: Optional[str],
	val_end: Optional[str],
) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
	"""æ ¹æ®æ—¥æœŸè¾¹ç•Œç”Ÿæˆ train/val/test ç´¢å¼•æ©ç ã€‚

	âš ï¸ æ—¶åºåˆ†å‰²ï¼ˆé˜²æ­¢look-ahead biasï¼‰ï¼š
	- train: [start, train_end]
	- val: (train_end, val_end]
	- test: (val_end, end]

	ç¡®ä¿ä¸¥æ ¼çš„æ—¶é—´é¡ºåºï¼Œä¸é‡å ã€‚
	"""
	d_min, d_max = sample_dates.min(), sample_dates.max()
	t_end = pd.to_datetime(train_end) if train_end else pd.to_datetime("2021-12-31")
	v_end = pd.to_datetime(val_end) if val_end else pd.to_datetime("2022-12-31")

	def mask(d1, d2):
		return (sample_dates >= d1) & (sample_dates <= d2)

	train_mask = mask(d_min, t_end)
	val_mask = mask(t_end + pd.Timedelta(days=1), v_end)
	test_mask = mask(v_end + pd.Timedelta(days=1), d_max)
	return train_mask, val_mask, test_mask


def compute_equity_curve(
	df_scores: pd.DataFrame,  # index: (date,ticker), column: pred, y
	quantile: float = 0.2,
) -> pd.Series:
	"""è®¡ç®—å¤šç©ºç»„åˆçš„å‡€å€¼æ›²çº¿ï¼ˆcumulative returnï¼‰ã€‚

	âš ï¸ æ—¶åºé€»è¾‘ï¼ˆç¡®ä¿æ— look-ahead biasï¼‰ï¼š
	- df_scores çš„ date ç´¢å¼•æ˜¯ä¿¡å·æ—¥æœŸ t
	- y åˆ—æ˜¯ [t+1, t+horizon] çš„æœªæ¥æ”¶ç›Šï¼ˆå·²ç»é€šè¿‡ shift(-horizon) å¯¹é½ï¼‰
	- æ¯æ—¥çš„æŒä»“æ”¶ç›Šç›´æ¥å¯¹åº”è¯¥æ—¥çš„ y å€¼ï¼ˆæ­£ç¡®åæ˜ äº†ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥çš„å®é™…æ”¶ç›Šï¼‰
	- pred æ˜¯åŸºäº t æ—¶åˆ»çš„ç‰¹å¾é¢„æµ‹çš„ï¼Œç”¨äº t+1 çš„äº¤æ˜“å†³ç­–

	Args:
		df_scores: é¢„æµ‹æ•°æ®ï¼ŒåŒ…å« predï¼ˆé¢„æµ‹ï¼‰å’Œ yï¼ˆå®é™…æ”¶ç›Šï¼‰
		quantile: å¤šç©ºç»„åˆçš„åˆ†ä½æ•°ï¼ˆå¦‚ 0.2 è¡¨ç¤ºåšå¤š top 20%ï¼Œåšç©º bottom 20%ï¼‰

	Returns:
		æ—¥æœŸç´¢å¼•çš„å‡€å€¼æ›²çº¿ï¼ˆç´¯è®¡æ”¶ç›Šï¼Œåˆå§‹å€¼ä¸º 1.0ï¼‰
	"""
	ls_daily_returns = []
	dates = []

	# æŒ‰æ—¥æœŸåˆ†ç»„è®¡ç®—æ¯æ—¥å¤šç©ºæ”¶ç›Š
	for d, g in df_scores.groupby(level=0):  # level=0 æ˜¯ date
		if len(g) < 20:  # æ ·æœ¬æ•°å¤ªå°‘åˆ™è·³è¿‡
			continue

		# æ ¹æ®é¢„æµ‹å€¼æ’åº
		ranks = g["pred"].rank(pct=True)

		# å¤šå¤´ï¼šé¢„æµ‹æ”¶ç›Šæœ€é«˜çš„ quantile åˆ†ä½
		top_ret = g.loc[ranks >= (1 - quantile), "y"].mean()
		# ç©ºå¤´ï¼šé¢„æµ‹æ”¶ç›Šæœ€ä½çš„ quantile åˆ†ä½
		bot_ret = g.loc[ranks <= quantile, "y"].mean()

		if np.isfinite(top_ret) and np.isfinite(bot_ret):
			# å¤šç©ºç»„åˆæ”¶ç›Š = å¤šå¤´æ”¶ç›Š - ç©ºå¤´æ”¶ç›Š
			ls_daily_returns.append(top_ret - bot_ret)
			dates.append(d)

	# è½¬æ¢ä¸º Series
	ls_series = pd.Series(ls_daily_returns, index=pd.DatetimeIndex(dates))

	# è®¡ç®—ç´¯è®¡å‡€å€¼ï¼ˆåˆå§‹å€¼ä¸º 1.0ï¼‰
	equity_curve = (1 + ls_series).cumprod()

	return equity_curve


def evaluate_predictions(
	df_scores: pd.DataFrame,  # index: (date,ticker), column: pred, y
	quantile: float = 0.2,
) -> Dict[str, float]:
	"""è¯„ä¼°é¢„æµ‹ï¼šRMSEã€ICã€ICIRã€ä»¥åŠç®€å•å¤šç©ºç»„åˆå¹´åŒ–æ”¶ç›Šä¸ IRã€‚

	Spearman IC æ‰‹å·¥å®ç°ï¼Œé¿å… SciPy ä¾èµ–ï¼ˆå¯¹æ¯ä¸ªæˆªé¢åšç§©ç›¸å…³ï¼‰ã€‚
	"""
	# RMSE
	diff = df_scores["pred"].values - df_scores["y"].values
	rmse = float(np.sqrt(np.mean(diff * diff))) if diff.size else np.nan

	ic_by_day: List[float] = []
	for d, g in df_scores.groupby(level=0):
		if len(g) < 5:
			continue
		rk_pred = g["pred"].rank(pct=True).values
		rk_y = g["y"].rank(pct=True).values
		if rk_pred.std() > 0 and rk_y.std() > 0:
			corr = np.corrcoef(rk_pred, rk_y)[0, 1]
			if np.isfinite(corr):
				ic_by_day.append(float(corr))
	ic_by_day = np.array(ic_by_day, dtype=float)
	ic_mean = float(np.nanmean(ic_by_day)) if ic_by_day.size else np.nan
	ic_ir = float(ic_mean / np.nanstd(ic_by_day)) if ic_by_day.size and np.nanstd(ic_by_day) > 0 else np.nan

	# å¤šç©º
	q = quantile
	ls_daily: List[float] = []
	for d, g in df_scores.groupby(level=0):
		if len(g) < 20:
			continue
		ranks = g["pred"].rank(pct=True)
		top_ret = g.loc[ranks >= (1 - q), "y"].mean()
		bot_ret = g.loc[ranks <= q, "y"].mean()
		if np.isfinite(top_ret) and np.isfinite(bot_ret):
			ls_daily.append(float((top_ret - bot_ret)))
	ls_daily = np.array(ls_daily, dtype=float)
	ann_ret = float(243 * np.nanmean(ls_daily)) if ls_daily.size else np.nan
	ann_vol = float(np.sqrt(243) * np.nanstd(ls_daily)) if ls_daily.size else np.nan
	ann_ir = float(ann_ret / ann_vol) if ls_daily.size and ann_vol > 0 else np.nan

	return {"rmse": rmse, "ic_mean": ic_mean, "ic_ir": ic_ir, "ls_ann_ret": ann_ret, "ls_ann_ir": ann_ir}


def plot_equity_curves(
	equity_data: Dict[str, Dict[str, pd.Series]],
	train_end: str,
	val_end: str,
	save_path: Optional[str] = None,
):
	"""ç»˜åˆ¶å¤šæ¨¡å‹å‡€å€¼æ›²çº¿å¯¹æ¯”å›¾ï¼ŒåŒºåˆ†æ ·æœ¬å†…ï¼ˆè®­ç»ƒ+éªŒè¯ï¼‰å’Œæ ·æœ¬å¤–ï¼ˆæµ‹è¯•ï¼‰ã€‚

	Args:
		equity_data: {
			"model_name": {
				"train": equity_curve_series,
				"val": equity_curve_series,
				"test": equity_curve_series,
			}
		}
		train_end: è®­ç»ƒé›†ç»“æŸæ—¥æœŸï¼ˆæ ·æœ¬å†…å¤–åˆ†ç•Œçº¿1ï¼‰
		val_end: éªŒè¯é›†ç»“æŸæ—¥æœŸï¼ˆæ ·æœ¬å†…å¤–åˆ†ç•Œçº¿2ï¼‰
		save_path: å›¾ç‰‡ä¿å­˜è·¯å¾„ï¼Œå¦‚ None åˆ™æ˜¾ç¤º
	"""
	try:
		import matplotlib.pyplot as plt
		import matplotlib.dates as mdates
	except ImportError:
		print("[warn] matplotlib æœªå®‰è£…ï¼Œæ— æ³•ç»˜åˆ¶å‡€å€¼æ›²çº¿")
		return

	plt.figure(figsize=(14, 7))
	ax = plt.gca()

	# é¢œè‰²æ–¹æ¡ˆ
	colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
	color_idx = 0

	# è½¬æ¢åˆ†ç•Œæ—¥æœŸ
	train_end_dt = pd.to_datetime(train_end)
	val_end_dt = pd.to_datetime(val_end)

	# ç»˜åˆ¶æ¯ä¸ªæ¨¡å‹çš„å‡€å€¼æ›²çº¿
	for model_name, splits in equity_data.items():
		color = colors[color_idx % len(colors)]
		color_idx += 1

		# åˆå¹¶ train, val, test çš„å‡€å€¼æ›²çº¿
		full_equity = pd.Series(dtype=float)

		for split_name in ["train", "val", "test"]:
			if split_name in splits and len(splits[split_name]) > 0:
				split_curve = splits[split_name]

				# å¦‚æœä¸æ˜¯ç¬¬ä¸€æ®µï¼Œéœ€è¦æ¥ç»­å‰ä¸€æ®µçš„æœ€ç»ˆå‡€å€¼
				if len(full_equity) > 0:
					last_value = full_equity.iloc[-1]
					# å°†å½“å‰æ®µçš„å‡€å€¼è°ƒæ•´ä¸ºæ¥ç»­ä¸Šä¸€æ®µ
					split_curve = split_curve / split_curve.iloc[0] * last_value

				full_equity = pd.concat([full_equity, split_curve])

		# ç»˜åˆ¶å®Œæ•´å‡€å€¼æ›²çº¿
		if len(full_equity) > 0:
			ax.plot(full_equity.index, full_equity.values,
				   label=model_name, color=color, linewidth=2, alpha=0.8)

	# æ·»åŠ æ ·æœ¬å†…å¤–åˆ†ç•Œçº¿
	ax.axvline(x=train_end_dt, color='gray', linestyle='--', linewidth=1.5, alpha=0.7, label='Train/Val Split')
	ax.axvline(x=val_end_dt, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Val/Test Split (æ ·æœ¬å¤–å¼€å§‹)')

	# æ·»åŠ åŒºåŸŸæ ‡æ³¨
	ymin, ymax = ax.get_ylim()
	ax.text(train_end_dt, ymax * 0.95, 'â† è®­ç»ƒæœŸ', ha='right', va='top', fontsize=10, alpha=0.7)
	ax.text(train_end_dt, ymax * 0.95, 'éªŒè¯æœŸ â†’', ha='left', va='top', fontsize=10, alpha=0.7)
	ax.text(val_end_dt, ymax * 0.95, 'â† æ ·æœ¬å†…', ha='right', va='top', fontsize=11, fontweight='bold', alpha=0.8)
	ax.text(val_end_dt, ymax * 0.95, 'æ ·æœ¬å¤– â†’', ha='left', va='top', fontsize=11, fontweight='bold',
		   color='red', alpha=0.8)

	# å›¾è¡¨ç¾åŒ–
	ax.set_xlabel('Date', fontsize=12)
	ax.set_ylabel('Cumulative Return (Net Value)', fontsize=12)
	ax.set_title('Multi-Model Equity Curves (In-Sample vs Out-of-Sample)', fontsize=14, fontweight='bold')
	ax.legend(loc='upper left', fontsize=10)
	ax.grid(True, alpha=0.3)

	# æ—¥æœŸæ ¼å¼åŒ–
	ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
	ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))
	plt.xticks(rotation=45)

	plt.tight_layout()

	if save_path:
		os.makedirs(os.path.dirname(save_path), exist_ok=True)
		plt.savefig(save_path, dpi=300, bbox_inches='tight')
		print(f"ğŸ“Š Equity curve saved to: {save_path}")
		plt.close()
	else:
		plt.show()


def train_xgboost(
	X: pd.DataFrame,
	y: pd.Series,
	sample_dates: pd.DatetimeIndex,
	train_mask: np.ndarray,
	val_mask: np.ndarray,
	test_mask: np.ndarray,
	xgb_params: Optional[dict] = None,
	model_out: Optional[str] = None,
	preds_out: Optional[str] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, pd.DataFrame]]:
	"""è®­ç»ƒ XGBoost å›å½’å¹¶è¯„ä¼° train/val/testï¼Œè¿”å›å„æ®µæŒ‡æ ‡å’Œé¢„æµ‹æ•°æ®ã€‚

	âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåŠæ—¶æ¸…ç†ä¸­é—´å˜é‡

	Returns:
		(metrics_dict, predictions_dict)
		- metrics_dict: {"train": {...}, "val": {...}, "test": {...}}
		- predictions_dict: {"train": df_scores, "val": df_scores, "test": df_scores}
	"""
	from xgboost import XGBRegressor

	xgb_params = xgb_params or {
		"n_estimators": 1000,
		"max_depth": 6,
		"learning_rate": 0.05,
		"subsample": 0.8,
		"colsample_bytree": 0.8,
		"reg_alpha": 0.0,
		"reg_lambda": 1.0,
		"objective": "reg:squarederror",
		"random_state": 42,
		"n_jobs": max(1, os.cpu_count() or 1),
		"tree_method": "hist",  # å†…å­˜ä¼˜åŒ–ï¼šä½¿ç”¨histogramæ–¹æ³•
	}

	# æ„é€ åˆ†æ®µç´¢å¼•
	# å°†æ—¥æœŸçº§åˆ«çš„æ©ç æ˜ å°„åˆ° (date,ticker) è¡Œçº§åˆ«
	date_level = X.index.get_level_values(0)
	map_train = pd.Series(train_mask, index=sample_dates)
	map_val = pd.Series(val_mask, index=sample_dates)
	map_test = pd.Series(test_mask, index=sample_dates)

	train_row_mask = map_train.reindex(date_level).to_numpy()
	val_row_mask = map_val.reindex(date_level).to_numpy()
	test_row_mask = map_test.reindex(date_level).to_numpy()

	idx = X.index
	train_idx = idx[train_row_mask]
	val_idx = idx[val_row_mask]
	test_idx = idx[test_row_mask]

	# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåˆ é™¤ä¸­é—´å˜é‡
	del map_train, map_val, map_test, train_row_mask, val_row_mask, test_row_mask, date_level
	gc.collect()

	model = XGBRegressor(**xgb_params)
	# ç®€åŒ–è®­ç»ƒï¼šä¸ä½¿ç”¨ early_stopping_roundsï¼ˆç‰ˆæœ¬å…¼å®¹é—®é¢˜ï¼‰ï¼Œåç»­å¯æ ¹æ® xgb.__version__ æ¡ä»¶æ·»åŠ 
	model.fit(
		X.loc[train_idx],
		y.loc[train_idx],
		eval_set=[(X.loc[val_idx], y.loc[val_idx])],
		verbose=False,
	)

	# é¢„æµ‹å¹¶è¯„ä¼°
	out = {}
	preds_dict = {}  # ä¿å­˜å®Œæ•´é¢„æµ‹æ•°æ®ç”¨äºç»˜å›¾
	for split_name, split_idx in [
		("train", train_idx),
		("val", val_idx),
		("test", test_idx),
	]:
		pred = model.predict(X.loc[split_idx])
		df_scores = pd.DataFrame({"pred": pred, "y": y.loc[split_idx].values}, index=split_idx)
		out[split_name] = evaluate_predictions(df_scores)
		preds_dict[split_name] = df_scores  # ä¿å­˜é¢„æµ‹æ•°æ®

		# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåŠæ—¶æ¸…ç†
		del pred
		gc.collect()

	# ä¿å­˜æ¨¡å‹ä¸é¢„æµ‹ï¼ˆæŒ‰ test æ®µï¼‰
	if model_out:
		os.makedirs(os.path.dirname(model_out), exist_ok=True)
		try:
			model.save_model(model_out)
		except Exception:
			# å¦‚æœ JSON ä¸å¯ç”¨ï¼Œå°è¯• pickle
			import pickle
			with open(model_out + ".pkl", "wb") as f:
				pickle.dump(model, f)

	if preds_out:
		os.makedirs(os.path.dirname(preds_out), exist_ok=True)
		test_pred = model.predict(X.loc[test_idx])
		df_scores = pd.DataFrame({"pred": test_pred, "y": y.loc[test_idx].values}, index=test_idx)
		# ä¿å­˜ä¸ºå‹ç¼© parquet
		try:
			df_scores.to_parquet(preds_out)
		except Exception:
			df_scores.to_csv(preds_out.replace(".parquet", ".csv"))

		# âš ï¸ å†…å­˜ä¼˜åŒ–
		del test_pred, df_scores
		gc.collect()

	return out, preds_dict


def train_lightgbm(
	X: pd.DataFrame,
	y: pd.Series,
	sample_dates: pd.DatetimeIndex,
	train_mask: np.ndarray,
	val_mask: np.ndarray,
	test_mask: np.ndarray,
	lgbm_params: Optional[dict] = None,
	preds_out: Optional[str] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, pd.DataFrame]]:
	try:
		from lightgbm import LGBMRegressor  # type: ignore
	except Exception as e:
		raise RuntimeError(f"LightGBM æœªå®‰è£…æˆ–ä¸å¯ç”¨: {e}")

	lgbm_params = lgbm_params or {
		"n_estimators": 1000,
		"max_depth": -1,
		"learning_rate": 0.05,
		"subsample": 0.8,
		"colsample_bytree": 0.8,
		"reg_alpha": 0.0,
		"reg_lambda": 1.0,
		"objective": "regression",
		"random_state": 42,
		"n_jobs": max(1, os.cpu_count() or 1),
	}

	# æ©ç æ˜ å°„
	date_level = X.index.get_level_values(0)
	map_train = pd.Series(train_mask, index=sample_dates)
	map_val = pd.Series(val_mask, index=sample_dates)
	map_test = pd.Series(test_mask, index=sample_dates)
	train_idx = X.index[map_train.reindex(date_level).to_numpy()]
	val_idx = X.index[map_val.reindex(date_level).to_numpy()]
	test_idx = X.index[map_test.reindex(date_level).to_numpy()]

	# âš ï¸ å†…å­˜ä¼˜åŒ–
	del map_train, map_val, map_test, date_level
	gc.collect()

	model = LGBMRegressor(**lgbm_params)
	model.fit(X.loc[train_idx], y.loc[train_idx])

	out = {}
	preds_dict = {}
	for split_name, split_idx in [("train", train_idx), ("val", val_idx), ("test", test_idx)]:
		pred = model.predict(X.loc[split_idx])
		df_scores = pd.DataFrame({"pred": pred, "y": y.loc[split_idx].values}, index=split_idx)
		out[split_name] = evaluate_predictions(df_scores)
		preds_dict[split_name] = df_scores

		# âš ï¸ å†…å­˜ä¼˜åŒ–
		del pred
		gc.collect()

	if preds_out:
		os.makedirs(os.path.dirname(preds_out), exist_ok=True)
		test_pred = model.predict(X.loc[test_idx])
		df_scores = pd.DataFrame({"pred": test_pred, "y": y.loc[test_idx].values}, index=test_idx)
		try:
			df_scores.to_parquet(preds_out)
		except Exception:
			df_scores.to_csv(preds_out.replace(".parquet", ".csv"))

		# âš ï¸ å†…å­˜ä¼˜åŒ–
		del test_pred, df_scores
		gc.collect()

	return out, preds_dict


def train_catboost(
	X: pd.DataFrame,
	y: pd.Series,
	sample_dates: pd.DatetimeIndex,
	train_mask: np.ndarray,
	val_mask: np.ndarray,
	test_mask: np.ndarray,
	cb_params: Optional[dict] = None,
	preds_out: Optional[str] = None,
) -> Tuple[Dict[str, Dict[str, float]], Dict[str, pd.DataFrame]]:
	try:
		from catboost import CatBoostRegressor  # type: ignore
	except Exception as e:
		raise RuntimeError(f"CatBoost æœªå®‰è£…æˆ–ä¸å¯ç”¨: {e}")

	cb_params = cb_params or {
		"iterations": 1000,
		"depth": 6,
		"learning_rate": 0.05,
		"l2_leaf_reg": 3.0,
		"loss_function": "RMSE",
		"random_seed": 42,
		"verbose": False,
		"thread_count": max(1, os.cpu_count() or 1),
	}

	date_level = X.index.get_level_values(0)
	map_train = pd.Series(train_mask, index=sample_dates)
	map_val = pd.Series(val_mask, index=sample_dates)
	map_test = pd.Series(test_mask, index=sample_dates)
	train_idx = X.index[map_train.reindex(date_level).to_numpy()]
	val_idx = X.index[map_val.reindex(date_level).to_numpy()]
	test_idx = X.index[map_test.reindex(date_level).to_numpy()]

	# âš ï¸ å†…å­˜ä¼˜åŒ–
	del map_train, map_val, map_test, date_level
	gc.collect()

	model = CatBoostRegressor(**cb_params)
	model.fit(X.loc[train_idx], y.loc[train_idx])

	out = {}
	preds_dict = {}
	for split_name, split_idx in [("train", train_idx), ("val", val_idx), ("test", test_idx)]:
		pred = model.predict(X.loc[split_idx])
		df_scores = pd.DataFrame({"pred": pred, "y": y.loc[split_idx].values}, index=split_idx)
		out[split_name] = evaluate_predictions(df_scores)
		preds_dict[split_name] = df_scores

		# âš ï¸ å†…å­˜ä¼˜åŒ–
		del pred
		gc.collect()

	if preds_out:
		os.makedirs(os.path.dirname(preds_out), exist_ok=True)
		test_pred = model.predict(X.loc[test_idx])
		df_scores = pd.DataFrame({"pred": test_pred, "y": y.loc[test_idx].values}, index=test_idx)
		try:
			df_scores.to_parquet(preds_out)
		except Exception:
			df_scores.to_csv(preds_out.replace(".parquet", ".csv"))

		# âš ï¸ å†…å­˜ä¼˜åŒ–
		del test_pred, df_scores
		gc.collect()

	return out, preds_dict


def main():
	parser = argparse.ArgumentParser(description="Train XGBoost on project factors")
	parser.add_argument("--modules", nargs="*", default=["alpha101", "gtja191", "chenhan_factor"], help="factor modules to use")
	parser.add_argument("--include", nargs="*", default=[], help="explicit factor class names to include")
	parser.add_argument("--exclude", nargs="*", default=[], help="factor class names to exclude")
	parser.add_argument("--start-date", type=str, default=None)
	parser.add_argument("--end-date", type=str, default=None)
	parser.add_argument("--universe", type=str, default="top75pct")
	parser.add_argument("--benchmark", type=str, default=None)
	parser.add_argument("--horizon", type=int, default=5, help="é¢„æµ‹æœŸé™ï¼ˆä»…ç”¨äºcumulativeæ ‡ç­¾ï¼‰")
	parser.add_argument("--label", type=str, default="cumulative",
						choices=["cumulative", "ret#2", "ret#5", "ret#20"],
						help="é¢„æµ‹æ ‡ç­¾ç±»å‹: cumulative=[t+1,t+h]ç´¯è®¡, ret#2=t+2/t+1, ret#5=t+5/t+1, ret#20=t+20/t+1")
	parser.add_argument("--train-end", type=str, default="2021-12-31")
	parser.add_argument("--val-end", type=str, default="2022-12-31")
	parser.add_argument("--no-zscore", action="store_true")
	parser.add_argument("--engine", type=str, choices=["xgb","lgbm","catboost"], default="xgb", help="é€‰æ‹©è®­ç»ƒæ¨¡å‹æ¡†æ¶")
	parser.add_argument("--rank-ic", action="store_true", help="è¿è¡Œæ‰€æœ‰å› å­å¹¶è¾“å‡º IC æ’åº JSON")
	parser.add_argument("--ic-horizon", type=int, default=5, help="ç”¨äº IC æ’åºçš„ horizon")
	parser.add_argument("--ic-threshold", type=float, default=0.0, help="IC é˜ˆå€¼ç­›é€‰ï¼šåªä¿ç•™ abs(rank_ic) > threshold çš„å› å­")
	parser.add_argument("--top-n", type=int, default=50, help="æ ¹æ® IC é€‰æ‹©å‰ N ä¸ªå› å­ç”¨äºè®­ç»ƒ")
	parser.add_argument("--persist-factors", action="store_true", help="æ˜¯å¦åœ¨å› å­æ¡†æ¶å†…éƒ¨è§¦å‘ä¿å­˜")
	parser.add_argument("--run-name", type=str, default=None, help="è‡ªå®šä¹‰è¿è¡Œåç§°ï¼ˆå¯é€‰ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³")
	parser.add_argument("--n-jobs", type=int, default=1, help="å¹¶è¡Œè®¡ç®—å› å­çš„è¿›ç¨‹æ•°ï¼ˆ1=ä¸²è¡Œï¼Œ>1=å¹¶è¡Œï¼‰")

	args = parser.parse_args()

	# åˆ›å»º run-specific æ–‡ä»¶å¤¹
	# æ ¼å¼: run_<engine>_<timestamp> æˆ– run_<engine>_<custom_name>
	import time
	timestamp = time.strftime("%Y%m%d_%H%M%S")
	if args.run_name:
		run_folder_name = f"run_{args.engine}_{args.run_name}"
	else:
		run_folder_name = f"run_{args.engine}_{timestamp}"

	run_dir = os.path.join(THIS_DIR, "runs", run_folder_name)
	os.makedirs(run_dir, exist_ok=True)
	print(f"[info] Run directory: {run_dir}")

	# æ›´æ–°æ‰€æœ‰è¾“å‡ºè·¯å¾„åˆ° run-specific ç›®å½•
	args.model_out = os.path.join(run_dir, f"{args.engine}_model.json")
	args.preds_out = os.path.join(run_dir, "test_preds.parquet")
	args.save_factor_json = os.path.join(run_dir, "factor_ic_rank.json")

	# å°†å­—ç¬¦ä¸²"None"è½¬æ¢ä¸ºPythonçš„None
	if args.universe == "None":
		args.universe = None
	if args.benchmark == "None":
		args.benchmark = None

	# å¦‚æœéœ€è¦å…ˆè¿è¡Œæ‰€æœ‰å› å­åš IC æ’åº
	factor_names_final: List[str] = []
	if args.rank_ic:
		print(f"[info] Running all factors for IC ranking (threshold: abs(IC) > {args.ic_threshold})...")
		print(f"[info] Using {args.n_jobs} parallel workers...")

		avg_ic_map, ok_names = run_all_factors_and_rank_ic(
			modules=args.modules,
			start_date=args.start_date,
			end_date=args.end_date,
			universe=args.universe,
			benchmark=args.benchmark,
			ic_horizon=args.ic_horizon,
			persist_factors=args.persist_factors,
			ic_threshold=args.ic_threshold,
			n_jobs=args.n_jobs,  # âš ï¸ å¹¶è¡ŒåŒ–
		)
		# æ’åºå¹¶ä¿å­˜ JSON
		sorted_items = sorted(avg_ic_map.items(), key=lambda kv: abs(kv[1]), reverse=True)
		os.makedirs(os.path.dirname(args.save_factor_json), exist_ok=True)
		with open(args.save_factor_json, "w", encoding="utf-8") as f:
			json.dump({"avg_ic": sorted_items, "all_run": ok_names}, f, ensure_ascii=False, indent=2)
		print(f"[info] IC ranking saved -> {args.save_factor_json}")
		# å–å‰ top-n
		factor_names_final = [name for name,_ in sorted_items[:args.top_n]]
		# è‹¥ç”¨æˆ·æœªæŒ‡å®š include åˆ™ç”¨ top-n
		if not args.include:
			args.include = factor_names_final
		else:
			# ç”¨æˆ·å·²æŒ‡å®š includeï¼Œåˆ™ä¸ top-n åšäº¤é›†å¢å¼ºå¯å¤ç”¨æ€§
			args.include = [nm for nm in args.include if nm in factor_names_final]
		print(f"[info] Selected top {len(args.include)} factors for training: {args.include[:10]}{'...' if len(args.include)>10 else ''}")

		# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šæ¸…ç†
		del avg_ic_map, ok_names, sorted_items
		gc.collect()

	# é€‰æ‹©ç”¨äºè®­ç»ƒçš„å› å­ç±»
	klass_map = select_factor_classes(args.modules, include=args.include, exclude=args.exclude)
	if not klass_map:
		raise SystemExit("æœªå‘ç°å¯ç”¨å› å­ç±» (after IC filtering)ï¼Œè¯·æ£€æŸ¥å‚æ•°ã€‚")
	print(f"Using {len(klass_map)} factors for model {args.engine}: {sorted(list(klass_map.keys()))[:10]}{'...' if len(klass_map)>10 else ''}")
	print(f"[info] Using {args.n_jobs} parallel workers for factor computation...")

	# è®¡ç®—å› å­ç‰¹å¾ï¼ˆâš ï¸ å¹¶è¡ŒåŒ–ï¼‰
	features, returns = compute_factor_features(
		klass_map,
		start_date=args.start_date,
		end_date=args.end_date,
		universe=args.universe,
		benchmark=args.benchmark,
		display=False,
		n_jobs=args.n_jobs,  # âš ï¸ å¹¶è¡ŒåŒ–
	)

	# æ„å»º ML æ•°æ®é›†
	X, y, sample_dates, label_name = build_ml_dataset(
		features,
		returns,
		horizon=args.horizon,
		label_type=args.label,
	)
	print(f"[info] Using label: {label_name}")

	# âš ï¸ å†…å­˜ä¼˜åŒ–ï¼šåˆ é™¤featureså’Œreturns
	del features, returns
	gc.collect()

	# æŒ‰æ—¶é—´åˆ‡åˆ†
	train_mask, val_mask, test_mask = time_split_indices(sample_dates, args.train_end, args.val_end)

	# âœ… Data Leakage é˜²æ­¢: Train ç»Ÿè®¡é‡è§„èŒƒåŒ–
	if not args.no_zscore:
		print("[info] Normalizing with train-only statistics (preventing data leakage)...")
		date_level = X.index.get_level_values(0)
		map_train = pd.Series(train_mask, index=sample_dates)
		train_row_mask = map_train.reindex(date_level).to_numpy()
		train_idx = X.index[train_row_mask]

		X = normalize_with_train_stats(X, train_idx, clip=5.0)

		# âš ï¸ å†…å­˜ä¼˜åŒ–
		del map_train, train_row_mask, date_level
		gc.collect()

	# è®­ç»ƒ & è¯„ä¼° & ä¿å­˜
	if args.engine == "xgb":
		metrics, predictions = train_xgboost(
			X, y, sample_dates, train_mask, val_mask, test_mask,
			xgb_params=None,
			model_out=args.model_out,
			preds_out=args.preds_out,
		)
	elif args.engine == "lgbm":
		metrics, predictions = train_lightgbm(
			X, y, sample_dates, train_mask, val_mask, test_mask,
			lgbm_params=None,
			preds_out=args.preds_out.replace("test_preds", "test_preds_lgbm"),
		)
	else:  # catboost
		metrics, predictions = train_catboost(
			X, y, sample_dates, train_mask, val_mask, test_mask,
			cb_params=None,
			preds_out=args.preds_out.replace("test_preds", "test_preds_catboost"),
		)

	# æ‰“å°ç»“æœå¹¶ä¿å­˜æŒ‡æ ‡
	pretty = json.dumps(metrics, ensure_ascii=False, indent=2)
	print(pretty)

	# ä¿å­˜ metrics åˆ° run æ–‡ä»¶å¤¹
	metrics_path = os.path.join(run_dir, "metrics.json")
	with open(metrics_path, "w", encoding="utf-8") as f:
		f.write(pretty)

	print(f"\nâœ… Metrics saved to: {metrics_path}")

	# è®¡ç®—å¹¶ç»˜åˆ¶å‡€å€¼æ›²çº¿
	print("\n[info] Computing equity curves...")
	equity_data = {}
	for split_name in ["train", "val", "test"]:
		if split_name in predictions and len(predictions[split_name]) > 0:
			equity_curve = compute_equity_curve(predictions[split_name], quantile=0.2)
			if split_name not in equity_data:
				equity_data[split_name] = {}
			equity_data[split_name] = equity_curve

	# ç»„ç»‡æ•°æ®ç”¨äºç»˜å›¾ï¼ˆæ”¯æŒå¤šæ¨¡å‹å¯¹æ¯”ï¼‰
	model_equity_data = {args.engine.upper(): equity_data}

	# ç»˜åˆ¶å‡€å€¼æ›²çº¿åˆ° run æ–‡ä»¶å¤¹
	equity_plot_path = os.path.join(run_dir, "equity_curve.png")
	plot_equity_curves(
		model_equity_data,
		train_end=args.train_end,
		val_end=args.val_end,
		save_path=equity_plot_path,
	)

	# ä¿å­˜ run é…ç½®ä¿¡æ¯
	run_config = {
		"run_name": run_folder_name,
		"timestamp": timestamp,
		"engine": args.engine,
		"num_factors": len(klass_map),
		"factor_names": sorted(list(klass_map.keys())),
		"label_type": args.label,
		"label_name": label_name,
		"horizon": args.horizon,
		"train_end": args.train_end,
		"val_end": args.val_end,
		"universe": args.universe,
		"benchmark": args.benchmark,
		"ic_threshold": args.ic_threshold if args.rank_ic else None,
		"zscore": not args.no_zscore,
		"n_jobs": args.n_jobs,
	}
	config_path = os.path.join(run_dir, "run_config.json")
	with open(config_path, "w", encoding="utf-8") as f:
		json.dump(run_config, f, ensure_ascii=False, indent=2)

	print(f"\nâœ… All results saved to: {run_dir}")
	print(f"   - Config: run_config.json")
	print(f"   - Metrics: metrics.json")
	print(f"   - Model: {os.path.basename(args.model_out)}")
	print(f"   - Predictions: {os.path.basename(args.preds_out)}")
	print(f"   - Equity curve: equity_curve.png")
	if args.rank_ic:
		print(f"   - IC ranking: factor_ic_rank.json")

	# âš ï¸ æœ€ç»ˆå†…å­˜æ¸…ç†
	gc.collect()
	print(f"\n[info] Memory optimization: {gc.get_count()} objects collected")


if __name__ == "__main__":
	main()
